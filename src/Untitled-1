Ingestion Results for /Users/ckhome/Desktop/baseline-codebase/src
Summary
Directory: src
Files analyzed: 42

Estimated tokens: 53.6k


Directory structure:
└── src/
    ├── __init__.py
    ├── evaluator.py
    ├── finetune.py
    ├── pretrain.py
    ├── run_test_example.sh
    ├── test_model.py
    ├── augmentations/
    │   ├── augmentation_composer.py
    │   ├── finetune_augmentation_presets.py
    │   └── mask.py
    ├── data/
    │   ├── datamodule.py
    │   ├── dataset.py
    │   ├── pretrain_split.py
    │   ├── task_configs.py
    │   ├── fomo-60k/
    │   │   └── preprocess.py
    │   └── preprocess/
    │       ├── fomo1.py
    │       ├── fomo2.py
    │       ├── fomo3.py
    │       └── run_preprocessing.py
    ├── inference/
    │   ├── apptainer_template.def
    │   ├── container_requirements.txt
    │   ├── predict_task1.py
    │   ├── predict_task2.py
    │   ├── predict_task3.py
    │   └── predict.py
    ├── models/
    │   ├── __init__.py
    │   ├── self_supervised.py
    │   ├── supervised_base.py
    │   ├── supervised_cls.py
    │   ├── supervised_reg.py
    │   ├── supervised_seg.py
    │   ├── conv_blocks/
    │   │   ├── __init__.py
    │   │   ├── blocks.py
    │   │   └── layers.py
    │   └── networks/
    │       ├── __init__.py
    │       ├── heads.py
    │       ├── mednext.py
    │       └── unet.py
    └── utils/
        ├── __init__.py
        ├── data_split.py
        ├── masking.py
        ├── utils.py
        └── visualisation.py


File Contents

===============================================
File: __init__.py
===============================================

import logging

logging.getLogger().setLevel(logging.INFO)


===============================================
File: evaluator.py
===============================================

import os
from functools import partial
from typing import Optional
import numpy as np
import nibabel as nib
import json
import sys
from batchgenerators.utilities.file_and_folder_operations import (
    subfiles,
    join,
    load_json,
    isfile,
)
from sklearn.metrics import confusion_matrix
from yucca.functional.evaluation.metrics import (
    dice,
    jaccard,
    sensitivity,
    precision,
    TP,
    FP,
    FN,
    total_pos_gt,
    total_pos_pred,
    volume_similarity,
)
from yucca.functional.evaluation.surface_metrics import get_surface_metrics_for_label
from tqdm import tqdm
from tqdm.contrib.concurrent import process_map


class Evaluator:
    def __init__(
        self,
        labels: list | int,
        folder_with_predictions,
        folder_with_ground_truth,
        raw_data_path,
        as_binary=False,
        do_surface_eval=False,
        overwrite: bool = False,
        num_workers=4,
        ignore_labels=["0"],
        include_cases: Optional[list] = None,
    ):
        self.name = "results"

        self.overwrite = overwrite
        self.surface_metrics = []

        self.metrics = {
            "Dice": dice,
            "Jaccard": jaccard,
            "Sensitivity": sensitivity,
            "Precision": precision,
            "Volume Similarity": volume_similarity,
            "True Positives": TP,
            "False Positives": FP,
            "False Negatives": FN,
            "Total Positives Ground Truth": total_pos_gt,
            "Total Positives Prediction": total_pos_pred,
        }

        if do_surface_eval:
            self.name += "_SURFACE"
            self.surface_metrics = [
                "Average Surface Distance",
            ]

        if isinstance(labels, int):
            self.labels = [str(i) for i in range(labels)]
        else:
            self.labels = labels
        self.as_binary = as_binary
        if self.as_binary:
            self.labels = ["0", "1"]
            self.name += "_BINARY"

        self.labels = np.sort(np.array(self.labels, dtype=np.uint8))
        self.ignore_labels = ignore_labels
        self.folder_with_predictions = folder_with_predictions
        self.folder_with_ground_truth = folder_with_ground_truth
        self.raw_data_path = raw_data_path

        self.outpath = join(self.folder_with_predictions, f"{self.name}.json")

        self.pred_subjects = subfiles(
            self.folder_with_predictions, suffix=".nii.gz", join=False
        )
        gt_subjects = subfiles(
            self.folder_with_ground_truth, suffix=".nii.gz", join=False
        )
        if include_cases is not None:
            include_cases_with_suffix = [f + ".nii.gz" for f in include_cases]
            self.gt_subjects = [
                f for f in gt_subjects if f in include_cases_with_suffix
            ]
        else:
            self.gt_subjects = gt_subjects

        self.num_workers = num_workers

        print(
            f"\n"
            f"STARTING EVALUATION \n"
            f"Folder with predictions: {self.folder_with_predictions}\n"
            f"Folder with ground truth: {self.folder_with_ground_truth}\n"
            f"Evaluating performance on labels: {self.labels}"
        )

    def sanity_checks(self):
        print("pred subjects", self.pred_subjects)
        print("gt subjects", self.gt_subjects)

        assert (
            self.pred_subjects <= self.gt_subjects
        ), "Ground Truth is missing for predicted scans"

        assert (
            self.gt_subjects <= self.pred_subjects
        ), "Prediction is missing for Ground Truth of scans"

        # Check if the Ground Truth directory is a subdirectory of a 'TaskXXX_MyTask' folder.
        # If so, there should be a dataset.json where we can double check that the supplied classes
        # match with the expected classes for the dataset.
        gt_is_task = [
            i for i in self.folder_with_ground_truth.split(os.sep) if "Task" in i
        ]
        if gt_is_task:
            gt_task = gt_is_task[0]
            dataset_json = join(self.raw_data_path, gt_task, "dataset.json")
            if isfile(dataset_json):
                dataset_json = load_json(dataset_json)
                print(
                    f"Labels found in dataset.json: {list(dataset_json['labels'].keys())}"
                )

    def run(self):
        if isfile(self.outpath) and not self.overwrite:
            print(f"Evaluation file already present in {self.outpath}. Skipping.")
        else:
            self.sanity_checks()
            results_dict = self.evaluate_folder()
            self.save_as_json(results_dict)

    def evaluate_folder(self):
        sys.stdout.flush()

        metric_names = list(self.metrics.keys()) + self.surface_metrics

        map_func = partial(
            process_case,
            pred_dir=self.folder_with_predictions,
            gt_dir=self.folder_with_ground_truth,
            labels=self.labels,
            metrics=self.metrics,
            calc_surface_metrics=len(self.surface_metrics) > 0,
            as_binary=self.as_binary,
            ignore_labels=self.ignore_labels,
        )

        results = process_map(
            map_func,
            self.pred_subjects,
            max_workers=self.num_workers,
            chunksize=1,
            desc="Map",
        )

        all_labels = [str(label) for label in self.labels] + ["all"]

        aggregated_results = reduce(
            results, labels=all_labels, metric_names=metric_names
        )

        mean_results = {}

        for label in all_labels:
            mean_results[str(label)] = {
                metric: (
                    round(np.nanmean(metric_vals), 4)
                    if not np.all(np.isnan(metric_vals))
                    else 0
                )
                for metric, metric_vals in aggregated_results[str(label)].items()
            }

        return dict(results) | {"mean": mean_results}

    def save_as_json(self, dict):
        print("Saving results.json at path: ", self.outpath)
        with open(self.outpath, "w") as f:
            json.dump(dict, f, default=float, indent=4)


def process_case(
    case,
    pred_dir,
    gt_dir,
    labels,
    metrics,
    calc_surface_metrics,
    as_binary,
    ignore_labels,
):
    pred_path = join(pred_dir, case)
    gt_path = join(gt_dir, case)

    results = {}

    pred = nib.load(pred_path)
    gt = nib.load(gt_path)

    if as_binary:
        cmat = confusion_matrix(
            np.around(gt.get_fdata().flatten()).astype(bool).astype(int),
            np.around(pred.get_fdata().flatten()).astype(bool).astype(int),
            labels=labels,
        )
    else:
        cmat = confusion_matrix(
            np.around(gt.get_fdata().flatten()).astype(int),
            np.around(pred.get_fdata().flatten()).astype(int),
            labels=labels,
        )

    tp_agg = 0
    fp_agg = 0
    fn_agg = 0
    tn_agg = 0

    for label in labels:
        label_results = {}

        tp = cmat[label, label]
        fp = sum(cmat[:, label]) - tp
        fn = sum(cmat[label, :]) - tp
        tn = np.sum(cmat) - tp - fp - fn  # often a redundant and meaningless metric

        label_str = str(label)
        if label_str not in ignore_labels:
            tp_agg += tp
            fp_agg += fp
            fn_agg += fn
            tn_agg += tn

        for metric, metric_function in metrics.items():
            label_results[metric] = round(metric_function(tp, fp, tn, fn), 4)

        if calc_surface_metrics:
            surface_metrics = get_surface_metrics_for_label(
                gt, pred, label, as_binary=as_binary
            )
            for surface_metric, val in surface_metrics.items():
                label_results[surface_metric] = round(val, 4)

        results[str(label)] = label_results

    results["all"] = {
        metric: round(metric_function(tp_agg, fp_agg, tn_agg, fn_agg), 4)
        for metric, metric_function in metrics.items()
    }

    # results["Prediction:"] = predpath
    # results["Ground Truth:"] = gtpath

    # `results` contains
    # {
    #   "0": { "dice": 0.8, "f1": 0.9 }
    #  ...
    #  }
    return (case, results)


def reduce(results_per_case, labels, metric_names):
    results_per_label = {}

    for label in labels:
        results_per_label[str(label)] = {}
        for metric in metric_names:
            results_per_label[str(label)][metric] = []

    for _, results in tqdm(results_per_case, "reduce"):
        for label, label_result in results.items():
            for metric, metric_val in label_result.items():
                results_per_label[str(label)][metric].append(metric_val)

    # `results_per_label` contains
    # {
    #   "0": {
    #     "dice": [0.8, 0.9, 0.2, ...]
    #     ...
    #   }
    # }
    return results_per_label


===============================================
File: finetune.py
===============================================

#!/usr/bin/env python

import argparse
import os
import logging
import torch
import lightning as L
import wandb
from lightning.pytorch.callbacks import ModelCheckpoint

from models.supervised_base import BaseSupervisedModel
from augmentations.finetune_augmentation_presets import (
    get_finetune_augmentation_params,
)
from utils.utils import (
    SimplePathConfig,
    setup_seed,
    find_checkpoint,
    load_pretrained_weights,
)

from batchgenerators.utilities.file_and_folder_operations import (
    maybe_mkdir_p as ensure_dir_exists,
)

from yucca.modules.data.augmentation.YuccaAugmentationComposer import (
    YuccaAugmentationComposer,
)
from yucca.modules.data.data_modules.YuccaDataModule import YuccaDataModule
from yucca.modules.callbacks.loggers import YuccaLogger
from yucca.modules.data.datasets.YuccaDataset import YuccaTrainDataset

from yucca.pipeline.configuration.split_data import get_split_config
from yucca.pipeline.configuration.configure_paths import detect_version
from data.dataset import FOMODataset
from data.task_configs import task1_config, task2_config, task3_config, task4_config


def get_task_config(taskid):
    if taskid == 1:
        task_cfg = task1_config
    elif taskid == 2:
        task_cfg = task2_config
    elif taskid == 3:
        task_cfg = task3_config
    elif taskid == 4:
        task_cfg = task4_config
    else:
        raise ValueError(f"Unknown taskid: {taskid}. Supported IDs are 1, 2, 3, and 4")

    return task_cfg


def main():
    logging.getLogger().setLevel(logging.INFO)

    parser = argparse.ArgumentParser()
    parser.add_argument(
        "--data_dir",
        type=str,
        help="Path to data directory",
        default="./data/preprocessed",
    )
    parser.add_argument(
        "--save_dir",
        type=str,
        help="Path to save models and results",
        default="./data/models",
    )
    parser.add_argument(
        "--pretrained_weights_path", type=str, help="Ckpt to finetune", default=None
    )
    # Model configuration
    parser.add_argument(
        "--model_name",
        type=str,
        default="unet_b",
        help="Model name defined in models.networks (unet_b, unet_xl, etc.)",
    )
    parser.add_argument("--precision", type=str, default="bf16-mixed")
    parser.add_argument("--patch_size", type=int, default=32)
    parser.add_argument("--learning_rate", type=float, default=1e-4)
    parser.add_argument("--compile", action="store_true")
    parser.add_argument("--compile_mode", type=str, default=None)
    # Hardware configuration
    parser.add_argument("--num_devices", type=int, default=1)
    parser.add_argument("--num_workers", type=int, default=8)
    parser.add_argument("--fast_dev_run", action="store_true")
    # Experiment tracking
    parser.add_argument("--new_version", action="store_true")
    parser.add_argument(
        "--augmentation_preset",
        type=str,
        choices=["all", "basic", "none"],
        default="basic",
    )
    # Training Parameters
    parser.add_argument("--epochs", type=int, default=500)
    parser.add_argument("--batch_size", type=int, default=2)
    parser.add_argument("--train_batches_per_epoch", type=int, default=100)
    # Task Configuration
    parser.add_argument(
        "--taskid",
        type=int,
        required=True,
        help="Task ID (1: FOMO1 classification, 2: FOMO2 classification, 3: FOMO3 regression, 4: PD classification)",
    )
    # Split Configuration
    parser.add_argument("--split_method", type=str, default="simple_train_val_split")
    parser.add_argument("--split_param", type=str, help="Split parameter", default=0.2)
    parser.add_argument(
        "--split_idx", type=int, default=0, help="Index of the split to use for kfold"
    )
    parser.add_argument(
        "--experiment", type=str, default="experiment", help="name of experiment"
    )
    args = parser.parse_args()

    assert args.patch_size % 8 == 0, (
        f"Patch size must be divisible by 8, got {args.patch_size}"
    )

    # Set up task configuration
    task_cfg = get_task_config(args.taskid)
    task_type = task_cfg["task_type"]
    task_name = task_cfg["task_name"]
    num_classes = task_cfg["num_classes"]
    modalities = len(task_cfg["modalities"])
    labels = task_cfg["labels"]

    run_type = "from_scratch" if args.pretrained_weights_path is None else "finetune"
    experiment_name = f"{run_type}_{args.experiment}_{args.taskid}"

    print(f"Using num_workers: {args.num_workers}, num_devices: {args.num_devices}")
    print(f"Task type: {task_type}")
    print("ARGS:", args)

    # Set up directory structure
    data_dir = args.data_dir
    train_data_dir = os.path.join(data_dir, task_name)

    # Path where logs, checkpoints etc is stored
    save_dir = os.path.join(args.save_dir, task_name, args.model_name)

    # Handle versioning for experiment tracking
    continue_from_most_recent = not args.new_version
    version = detect_version(save_dir, continue_from_most_recent)
    version_dir = os.path.join(save_dir, f"version_{version}")
    ensure_dir_exists(version_dir)

    # Create dataset splits
    if args.split_method == "kfold":
        split_param = int(args.split_param)
    elif args.split_method == "simple_train_val_split":
        split_param = float(args.split_param)
    else:
        split_param = args.split_param

    path_config = SimplePathConfig(train_data_dir=train_data_dir)
    splits_config = get_split_config(
        method=args.split_method,
        param=split_param,
        path_config=path_config,
    )

    # Set up seed for reproducability
    seed = setup_seed(continue_from_most_recent)
    # Look for existing checkpoint if continuing training
    ckpt_path = find_checkpoint(version_dir, continue_from_most_recent)

    # Calculate training metrics
    effective_batch_size = args.num_devices * args.batch_size
    train_dataset_size = len(splits_config.train(args.split_idx))
    val_dataset_size = len(splits_config.val(args.split_idx))
    max_iterations = int(args.epochs * args.train_batches_per_epoch)

    # the config contains all the parameters needed for training and is used by lightning module, data module, and trainer
    config = {
        # Task information
        "task": task_name,
        "task_id": args.taskid,
        "task_type": task_type,
        "experiment": experiment_name,
        "model_name": args.model_name,
        "model_dimensions": "3D",
        "run_type": run_type,
        # Split configuration
        "split_method": args.split_method,
        "split_param": split_param,
        "split_idx": args.split_idx,
        # Directories
        "save_dir": save_dir,
        "train_data_dir": train_data_dir,
        "version_dir": version_dir,
        "version": version,
        # Checkpoint
        "ckpt_path": ckpt_path,
        "pretrained_weights_path": args.pretrained_weights_path,
        # Reproducibility
        "seed": seed,
        # Dataset properties
        "num_classes": num_classes,
        "num_modalities": modalities,
        "image_extension": ".npy",
        "allow_missing_modalities": False,
        "labels": labels,
        # Training parameters
        "batch_size": args.batch_size,
        "learning_rate": args.learning_rate,
        "patch_size": (args.patch_size,) * 3,
        "precision": args.precision,
        "augmentation_preset": args.augmentation_preset,
        "epochs": args.epochs,
        "train_batches_per_epoch": args.train_batches_per_epoch,
        "effective_batch_size": effective_batch_size,
        # Dataset metrics
        "train_dataset_size": train_dataset_size,
        "val_dataset_size": val_dataset_size,
        "max_iterations": max_iterations,
        # Hardware settings
        "num_devices": args.num_devices,
        "num_workers": args.num_workers,
        # Model compilation
        "compile": args.compile,
        "compile_mode": args.compile_mode,
        # Trainer specific params
        "fast_dev_run": args.fast_dev_run,
    }

    # Create checkpoint callback for saving models
    checkpoint_callback = ModelCheckpoint(
        every_n_epochs=10,
        save_top_k=1,
        filename="last",
        enable_version_counter=False,
    )
    callbacks = [checkpoint_callback]

    # Create logger for metrics
    yucca_logger = YuccaLogger(
        save_dir=save_dir,
        version=version,
        steps_per_epoch=args.train_batches_per_epoch,
    )

    # Create wandb logger for Lightning
    wandb_logger = L.pytorch.loggers.WandbLogger(
        project="fomo-finetuning",
        name=f"{config['experiment']}_version_{config['version']}",
        log_model=True,
    )

    # Set up loggers
    loggers = [yucca_logger, wandb_logger]

    # Configure augmentations based on preset
    aug_params = get_finetune_augmentation_params(args.augmentation_preset)
    # we use the cls augmentatoin preset for regression
    tt_preset = "classification" if task_type == "regression" else task_type
    augmenter = YuccaAugmentationComposer(
        patch_size=config["patch_size"],
        task_type_preset=tt_preset,
        parameter_dict=aug_params,
        deep_supervision=False,
    )

    # Create the data module that handles loading and batching
    data_module = YuccaDataModule(
        train_dataset_class=(
            YuccaTrainDataset if task_type == "segmentation" else FOMODataset
        ),
        composed_train_transforms=augmenter.train_transforms,
        composed_val_transforms=augmenter.val_transforms,
        patch_size=config["patch_size"],
        batch_size=config["batch_size"],
        train_data_dir=config["train_data_dir"],
        image_extension=config["image_extension"],
        task_type=config["task_type"],
        splits_config=splits_config,
        split_idx=config["split_idx"],
        num_workers=args.num_workers,
        val_sampler=None,
    )

    # Print dataset information
    print("Train dataset: ", data_module.splits_config.train(config["split_idx"]))
    print("Val dataset: ", data_module.splits_config.val(config["split_idx"]))
    print("Run type: ", run_type)
    print(
        f"Starting training with {max_iterations} max iterations over {args.epochs} epochs "
        f"with train dataset of size {train_dataset_size} datapoints and val dataset of size {val_dataset_size} "
        f"and effective batch size of {effective_batch_size}"
    )

    # Create model and trainer
    model = BaseSupervisedModel.create(
        task_type=task_type,
        config=config,
        learning_rate=args.learning_rate,
        do_compile=args.compile,
        compile_mode="default" if args.compile_mode is None else args.compile_mode,
    )

    # Create Lightning trainer
    trainer = L.Trainer(
        callbacks=callbacks,
        logger=loggers,
        accelerator="auto" if torch.cuda.is_available() else "cpu",
        strategy="auto",
        num_nodes=1,
        devices=args.num_devices,
        default_root_dir=save_dir,
        max_epochs=args.epochs,
        limit_train_batches=args.train_batches_per_epoch,
        precision=args.precision,
        fast_dev_run=args.fast_dev_run,
    )

    # Load pretrained weights if finetuning
    if run_type == "finetune":
        print("Transferring weights for finetuning")
        print(f"Checkpoint path: {ckpt_path}")
        assert ckpt_path is None, (
            "Error: You're attempting to load pretrained weights while "
            "simultaneously continuing from a checkpoint. This creates "
            "conflicting weight sources. Use either --pretrained_weights_path "
            "for finetuning OR continue training without the --new_version flag, "
            "but not both."
        )
        # Load and adjust weights from pretrained model
        state_dict = load_pretrained_weights(args.pretrained_weights_path, args.compile)

        # Transfer weights to new model
        num_successful_weights_transferred = model.load_state_dict(
            state_dict=state_dict, strict=False
        )
        assert num_successful_weights_transferred > 0, (
            "No weights were successfully transferred"
        )
    else:
        print("Training from scratch, no weights will be transferred")

    # Start training
    trainer.fit(model=model, datamodule=data_module, ckpt_path="last")
    wandb.finish()


if __name__ == "__main__":
    main()


===============================================
File: pretrain.py
===============================================

#!/usr/bin/env python

import os
import torch
import lightning as L
import argparse
import warnings
import wandb
from lightning.pytorch.callbacks import ModelCheckpoint
from batchgenerators.utilities.file_and_folder_operations import (
    maybe_mkdir_p as ensure_dir_exists,
)

from models.self_supervised import SelfSupervisedModel
from augmentations.augmentation_composer import (
    get_pretrain_augmentations,
    get_val_augmentations,
)
from data.datamodule import PretrainDataModule
from data.pretrain_split import get_pretrain_split_config
from yucca.pipeline.configuration.configure_paths import detect_version
from utils.utils import setup_seed, SimplePathConfig


def main():
    warnings.filterwarnings("ignore")
    parser = argparse.ArgumentParser()

    parser.add_argument(
        "--save_dir",
        type=str,
        required=True,
        help="Path to output directory for models and logs",
    )
    parser.add_argument(
        "--pretrain_data_dir",
        type=str,
        required=True,
        help="Path to pretraining data directory",
    )
    parser.add_argument("--model_name", type=str, default="unet_b_lw_dec")
    parser.add_argument("--epochs", type=int, default=100)
    parser.add_argument("--warmup_epochs", type=int, default=5)
    parser.add_argument("--precision", type=str, default="bf16-mixed")
    parser.add_argument(
        "--mask_patch_size",
        type=int,
        default=4,
        help="i.e. MAE patch size, the masking unit.",
    )
    parser.add_argument("--mask_ratio", type=float, default=0.6)
    parser.add_argument(
        "--patch_size",
        type=int,
        default=64,
        help="The patch size of the 3D patches extracted from the whole volume.",
    )
    parser.add_argument("--batch_size", type=int, default=8)
    parser.add_argument("--learning_rate", type=float, default=1e-4)
    parser.add_argument("--fast_dev_run", action="store_true")
    parser.add_argument("--num_devices", type=int, default=1)
    parser.add_argument("--num_workers", type=int, default=8)
    parser.add_argument("--compile", action="store_true")
    parser.add_argument("--compile_mode", type=str, default=None)
    parser.add_argument("--new_version", action="store_true")
    parser.add_argument("--optimizer", type=str, default="AdamW")

    parser.add_argument(
        "--augmentation_preset",
        type=str,
        choices=["all", "basic", "none"],
        default="none",
    )
    parser.add_argument("--loss_masked_tokens_only", default=False, action="store_true")

    parser.add_argument("--limit_val_batches", type=int, default=None)
    parser.add_argument("--limit_train_batches", type=int, default=None)
    parser.add_argument("--accumulate_grad_batches", type=int, default=1)
    parser.add_argument("--overfit_batches", type=int, default=0)
    parser.add_argument("--check_val_every_n_epoch", type=int, default=None)
    parser.add_argument(
        "--checkpoint_every_n_epochs",
        type=int,
        default=1,
        help="Save a checkpoint every N epochs",
    )

    parser.add_argument(
        "--experiment", type=str, default="base_experiment", help="name of experiment"
    )
    parser.add_argument(
        "--checkpoint_path", 
        type=str, 
        default=None,
        help="Path to specific checkpoint file to resume from. If not specified, will resume from last checkpoint."
    )

    args = parser.parse_args()

    assert args.patch_size % 8 == 0, args.patch_size
    assert args.mask_patch_size < args.patch_size

    print(f"Using num_workers: {args.num_workers}, num_devices: {args.num_devices}")
    print("ARGS:", args)

    # Set up directory structure
    train_data_dir = args.pretrain_data_dir

    # Path where logs, checkpoints etc is stored
    save_dir = os.path.join(
        args.save_dir, "models", os.path.basename(train_data_dir), args.model_name
    )
    versions_dir = os.path.join(save_dir, "versions")
    continue_from_most_recent = not args.new_version
    version = detect_version(versions_dir, continue_from_most_recent)
    version_dir = os.path.join(versions_dir, f"version_{version}")
    ensure_dir_exists(version_dir)

    # Configure training environment
    seed = setup_seed(continue_from_most_recent)

    # Create dataset splits
    path_config = SimplePathConfig(train_data_dir=train_data_dir)
    splits_config = get_pretrain_split_config(
        method="simple_train_val_split",
        idx=0,
        split_ratio=0.01,  # We use 1% of data for validation split
        path_config=path_config,
    )

    # configuration dictionary
    config = {
        # Experiment information
        "experiment": args.experiment,
        "model_name": args.model_name,
        "model_dimensions": "3D",
        "task_type": "self-supervised",
        "version": version,
        # Directories
        "save_dir": save_dir,
        "train_data_dir": train_data_dir,
        "version_dir": version_dir,
        # Reproducibility
        "seed": seed,
        # Model parameters
        "patch_size": (args.patch_size,) * 3,
        "mask_patch_size": args.mask_patch_size,
        "mask_ratio": args.mask_ratio,
        "input_channels": 1,
        "num_classes": 1,
        "should_compile": args.compile,
        "compile_mode": args.compile_mode,
        "rec_loss_masked_only": args.loss_masked_tokens_only,
        # Training parameters
        "batch_size": args.batch_size,
        "epochs": args.epochs,
        "warmup_epochs": args.warmup_epochs,
        "learning_rate": args.learning_rate,
        "optimizer": args.optimizer,
        "effective_batch_size": args.accumulate_grad_batches
        * args.num_devices
        * args.batch_size,
        "precision": args.precision,
        "augmentation_preset": args.augmentation_preset,
        # Hardware configuration
        "num_devices": args.num_devices,
        "num_workers": args.num_workers,
        # Dataset metrics
        "train_dataset_size": len(splits_config.train(0)),
        "val_dataset_size": len(splits_config.val(0)),
        # Trainer specific params  
        "fast_dev_run": args.fast_dev_run,
        "limit_val_batches": args.limit_val_batches,
        "limit_train_batches": args.limit_train_batches,
        "overfit_batches": args.overfit_batches,
        "check_val_every_n_epoch": args.check_val_every_n_epoch,
        "accumulate_grad_batches": args.accumulate_grad_batches,
        "gradient_clip_val": 1.0
    }

    # Calculate training metrics based on the config
    steps_per_epoch = (
        int(config["train_dataset_size"] / config["effective_batch_size"])
        if config["overfit_batches"] == 0
        else config["overfit_batches"]
    )
    max_iterations = int(config["epochs"] * steps_per_epoch)
    config["steps_per_epoch"] = steps_per_epoch
    config["max_iterations"] = max_iterations

    print(
        f"Starting training with {max_iterations} max iterations over {config['epochs']} epochs "
        f"with {config['train_dataset_size']} training datapoints, {config['val_dataset_size']} validation datapoints, "
        f"and an effective batch size of {config['effective_batch_size']}"
    )

    # Set up data augmentation and datamodule
    train_transforms = get_pretrain_augmentations(
        config["patch_size"], args.augmentation_preset
    )
    val_transforms = get_val_augmentations()

    data = PretrainDataModule(
        patch_size=config["patch_size"],
        batch_size=config["batch_size"],
        num_workers=config["num_workers"],
        splits_config=splits_config,
        split_idx=0,
        train_data_dir=train_data_dir,
        composed_train_transforms=train_transforms,
        composed_val_transforms=val_transforms,
    )

    # Create model and trainer
    model = SelfSupervisedModel(
        model_name=config["model_name"],
        config=config,
        epochs=config["epochs"],
        warmup_epochs=config["warmup_epochs"],
        learning_rate=config["learning_rate"],
        optimizer=config["optimizer"],
        steps_per_epoch=config["steps_per_epoch"],
        num_classes=config["num_classes"],
        input_channels=config["input_channels"],
        patch_size=config["patch_size"],
        mask_patch_size=config["mask_patch_size"],
        mask_ratio=config["mask_ratio"],
        should_compile=config["should_compile"],
        compile_mode=config["compile_mode"],
        rec_loss_masked_only=config["rec_loss_masked_only"],
    )

    # Initialize wandb logging
    wandb.init(
        project="fomo-pretraining",
        name=f"{config['experiment']}_version_{config['version']}",
    )

    # Create wandb logger for Lightning
    wandb_logger = L.pytorch.loggers.WandbLogger(
        project="fomo-pretraining",
        name=f"{config['experiment']}_version_{config['version']}",
        log_model=True,
    )

    # Create checkpoint callback
    checkpoint_callback = ModelCheckpoint(
        dirpath=version_dir,
        filename="{epoch:02d}",
        every_n_epochs=args.checkpoint_every_n_epochs,
        save_last=True,
    )
    callbacks = [checkpoint_callback]

    trainer = L.Trainer(
        logger=wandb_logger,
        callbacks=callbacks,
        accelerator="auto" if torch.cuda.is_available() else "cpu",
        strategy="ddp" if config["num_devices"] > 1 else "auto",
        num_nodes=1,
        devices=config["num_devices"],
        default_root_dir=config["save_dir"],
        max_epochs=config["epochs"],
        precision=config["precision"],
        fast_dev_run=config["fast_dev_run"],
        limit_val_batches=config["limit_val_batches" ],
        limit_train_batches=config["limit_train_batches"],
        overfit_batches=config["overfit_batches"],
        check_val_every_n_epoch=config["check_val_every_n_epoch"],
        num_sanity_val_steps=0 if config["overfit_batches"] > 0 else 2,
        accumulate_grad_batches=config["accumulate_grad_batches"],
    )

    # Determine checkpoint path
    ckpt_path = args.checkpoint_path if args.checkpoint_path else "last"
    trainer.fit(model=model, datamodule=data, ckpt_path=ckpt_path)
    trainer.print(f"Memory used: {torch.cuda.max_memory_allocated() / 1e9:.02f} GB")

    # Close the wandb logging session
    wandb.finish()


if __name__ == "__main__":
    main()


===============================================
File: run_test_example.sh
===============================================

#!/bin/bash

# Example script to run the test_model.py script for different tasks

# Set your checkpoint path here
CHECKPOINT_PATH="./data/models/Task001_FOMO1/unet_b/version_0/checkpoints/last.ckpt"

# Example 1: Test FOMO1 classification task (Task 1)
# echo "Testing FOMO1 classification task..."
# python src/test_model.py \
#     --checkpoint_path "$CHECKPOINT_PATH" \
#     --taskid 1 \
#     --data_dir "./data/preprocessed" \
#     --output_dir "./test_results" \
#     --batch_size 4 \
#     --num_workers 4 \
#     --augmentation_preset "none" \
#     --seed 42

# Example 2: Test FOMO2 classification task (Task 2)
# echo "Testing FOMO2 classification task..."
# python src/test_model.py \
#     --checkpoint_path "./data/models/Task002_FOMO2/unet_b/version_0/checkpoints/last.ckpt" \
#     --taskid 2 \
#     --data_dir "./data/preprocessed" \
#     --output_dir "./test_results" \
#     --batch_size 4 \
#     --num_workers 4 \
#     --augmentation_preset "none" \
#     --seed 42

# Example 3: Test FOMO3 regression task (Task 3)
# echo "Testing FOMO3 regression task..."
# python src/test_model.py \
#     --checkpoint_path "./data/models/Task003_FOMO3/unet_b/version_0/checkpoints/last.ckpt" \
#     --taskid 3 \
#     --data_dir "./data/preprocessed" \
#     --output_dir "./test_results" \
#     --batch_size 4 \
#     --num_workers 4 \
#     --augmentation_preset "none" \
#     --seed 42

# Example 4: Test PD classification task (Task 4)
echo "Testing PD classification task..."
python src/test_model.py \
    --checkpoint_path "/local_scratch/crc/models/finetuned/Task004_PD/unet_b/version_0/checkpoints/last.ckpt" \
    --taskid 4 \
    --data_dir "/local_scratch/crc/data/pd_temp/val/" \
    --output_dir "./test_results" \
    --batch_size 4 \
    --model_name "unet_b" \
    --patch_size 32 \
    --augmentation_preset "none" \
    --seed 42

echo "Testing completed!" 

===============================================
File: test_model.py
===============================================

#!/usr/bin/env python

import argparse
import os
import logging
import torch
import numpy as np
import json
from sklearn.metrics import roc_auc_score, accuracy_score, precision_score, recall_score, f1_score
from tqdm import tqdm
import lightning as L
from torch.utils.data import DataLoader

from models.supervised_base import BaseSupervisedModel
from data.dataset import FOMODataset
from data.task_configs import task1_config, task2_config, task3_config, task4_config
from utils.utils import SimplePathConfig, setup_seed
from yucca.pipeline.configuration.split_data import get_split_config
from yucca.modules.data.augmentation.YuccaAugmentationComposer import YuccaAugmentationComposer
from augmentations.finetune_augmentation_presets import get_finetune_augmentation_params


def get_task_config(taskid):
    """Get task configuration based on task ID."""
    if taskid == 1:
        task_cfg = task1_config
    elif taskid == 2:
        task_cfg = task2_config
    elif taskid == 3:
        task_cfg = task3_config
    elif taskid == 4:
        task_cfg = task4_config
    else:
        raise ValueError(f"Unknown taskid: {taskid}. Supported IDs are 1, 2, 3, and 4")
    return task_cfg


def load_model_from_checkpoint(checkpoint_path, task_type, config):
    """Load model from checkpoint."""
    # Create model instance
    model = BaseSupervisedModel.create(
        task_type=task_type,
        config=config,
        learning_rate=1e-4,  # Not used for inference
        do_compile=False,
        compile_mode="default",
    )
    
    # Load checkpoint
    checkpoint = torch.load(checkpoint_path, map_location='cpu')
    
    # Handle different checkpoint formats
    if 'state_dict' in checkpoint:
        state_dict = checkpoint['state_dict']
    else:
        state_dict = checkpoint
    
    # Load state dict
    model.load_state_dict(state_dict, strict=False)
    model.eval()
    
    return model


def create_test_dataset(data_dir, task_name, patch_size, task_type, augmentation_preset):
    """Create test dataset using all available data."""
    # Set up path configuration
    train_data_dir = data_dir
    path_config = SimplePathConfig(train_data_dir=train_data_dir)
    
    # Get all samples from the data directory
    from batchgenerators.utilities.file_and_folder_operations import subfiles
    all_files = subfiles(train_data_dir, suffix=".npy", join=False)
    # Remove .npy extension to get base filenames and create full paths
    test_samples = [os.path.join(train_data_dir, f[:-4]) for f in all_files]
    
    # Configure augmentations (validation transforms only)
    aug_params = get_finetune_augmentation_params(augmentation_preset)
    tt_preset = "classification" if task_type == "regression" else task_type
    augmenter = YuccaAugmentationComposer(
        patch_size=(patch_size,) * 3,
        task_type_preset=tt_preset,
        parameter_dict=aug_params,
        deep_supervision=False,
    )
    
    # Create dataset
    if task_type == "segmentation":
        from yucca.modules.data.datasets.YuccaDataset import YuccaTrainDataset
        dataset = YuccaTrainDataset(
            samples=test_samples,
            patch_size=(patch_size,) * 3,
            composed_transforms=augmenter.val_transforms,
            task_type=task_type,
        )
    else:
        dataset = FOMODataset(
            samples=test_samples,
            patch_size=(patch_size,) * 3,
            composed_transforms=augmenter.val_transforms,
            task_type=task_type,
        )
    
    return dataset, test_samples


def evaluate_classification(model, dataloader, device, num_classes):
    """Evaluate classification model and compute metrics including AUROC."""
    model.to(device)
    model.eval()
    
    all_predictions = []
    all_probabilities = []
    all_targets = []
    all_file_paths = []
    
    with torch.no_grad():
        for batch in tqdm(dataloader, desc="Evaluating"):
            # Move batch to device
            batch = {k: v.to(device) if isinstance(v, torch.Tensor) else v for k, v in batch.items()}
            
            # Get inputs and targets
            inputs = batch["image"]
            targets = batch["label"]
            file_paths = batch["file_path"]
            
            # Forward pass
            outputs = model(inputs)
            
            
            probabilities = torch.softmax(outputs, dim=1)
            predictions = torch.argmax(probabilities, dim=1)
            # For AUROC, we need probabilities for each class
            prob_positive = probabilities
            
            # Store results
            all_predictions.extend(predictions.cpu().numpy())
            all_probabilities.extend(prob_positive.cpu().numpy())
            all_targets.extend(targets.cpu().numpy())
            all_file_paths.extend(file_paths)
            
            # Debug first batch
            if len(all_targets) == len(predictions):
                print(f"First batch - targets shape: {targets.shape}, predictions shape: {predictions.shape}")
                print(f"First batch - targets: {targets.cpu().numpy()}")
                print(f"First batch - predictions: {predictions.cpu().numpy()}")
    
    # Convert to numpy arrays and ensure proper shapes
    all_predictions = np.array(all_predictions)
    all_probabilities = np.array(all_probabilities)
    all_targets = np.array(all_targets)
    
    # Ensure targets are 1D
    if all_targets.ndim > 1:
        all_targets = all_targets.squeeze()
    
    # Ensure predictions are 1D
    if all_predictions.ndim > 1:
        all_predictions = all_predictions.squeeze()
    
    # Ensure probabilities have correct shape
    if all_probabilities.ndim == 1:
        # Binary case - probabilities should be 1D
        pass
    else:
        # Multi-class case - probabilities should be 2D (samples, classes)
        if all_probabilities.ndim > 2:
            all_probabilities = all_probabilities.squeeze()
    
    print(f"Targets shape: {all_targets.shape}, dtype: {all_targets.dtype}")
    print(f"Predictions shape: {all_predictions.shape}, dtype: {all_predictions.dtype}")
    print(f"Probabilities shape: {all_probabilities.shape}, dtype: {all_probabilities.dtype}")
    print(f"Target values: {np.unique(all_targets)}")
    print(f"Prediction values: {np.unique(all_predictions)}")
    print(f"Target sample: {all_targets[:5]}")
    print(f"Prediction sample: {all_predictions[:5]}")
    print(f"Target min/max: {all_targets.min()}/{all_targets.max()}")
    print(f"Prediction min/max: {all_predictions.min()}/{all_predictions.max()}")
    
    # Compute metrics
    metrics = {}
    
    # Ensure both targets and predictions are 1D arrays with same dtype
    # Force flatten and convert to int
    y_true = all_targets.flatten().astype(int)
    y_pred = all_predictions.flatten().astype(int)
    
    # Additional safety check - ensure they are truly 1D
    if y_true.ndim != 1:
        y_true = y_true.ravel()
    if y_pred.ndim != 1:
        y_pred = y_pred.ravel()
    
    print(f"After conversion - y_true shape: {y_true.shape}, dtype: {y_true.dtype}")
    print(f"After conversion - y_pred shape: {y_pred.shape}, dtype: {y_pred.dtype}")
    print(f"After conversion - y_true sample: {y_true[:5]}")
    print(f"After conversion - y_pred sample: {y_pred[:5]}")
    print(f"After conversion - y_true unique: {np.unique(y_true)}")
    print(f"After conversion - y_pred unique: {np.unique(y_pred)}")
    print(f"After conversion - y_true is 1D: {y_true.ndim == 1}")
    print(f"After conversion - y_pred is 1D: {y_pred.ndim == 1}")
    
    # Final safety check - ensure they are exactly the same format
    assert y_true.ndim == 1, f"y_true is not 1D: shape {y_true.shape}"
    assert y_pred.ndim == 1, f"y_pred is not 1D: shape {y_pred.shape}"
    assert y_true.dtype == y_pred.dtype, f"dtype mismatch: {y_true.dtype} vs {y_pred.dtype}"
    
    # Basic classification metrics with individual error handling
    try:
        metrics['accuracy'] = accuracy_score(y_true, y_pred)
        print("Accuracy computed successfully")
    except Exception as e:
        print(f"Error computing accuracy: {e}")
        metrics['accuracy'] = None
    
    try:
        metrics['precision'] = precision_score(y_true, y_pred, average='macro', zero_division=0)
        print("Precision computed successfully")
    except Exception as e:
        print(f"Error computing precision: {e}")
        metrics['precision'] = None
    
    try:
        metrics['recall'] = recall_score(y_true, y_pred, average='macro', zero_division=0)
        print("Recall computed successfully")
    except Exception as e:
        print(f"Error computing recall: {e}")
        metrics['recall'] = None
    
    try:
        metrics['f1'] = f1_score(y_true, y_pred, average='macro', zero_division=0)
        print("F1 computed successfully")
    except Exception as e:
        print(f"Error computing F1: {e}")
        metrics['f1'] = None
    
    # AUROC
    if num_classes == 2:
        # Binary classification
        try:
            # Ensure we have binary targets (0 and 1) and proper probability format
            y_true_binary = y_true
            y_score_binary = all_probabilities.ravel().astype(float)
            
            # Check if we have both classes
            unique_classes = np.unique(y_true_binary)
            if len(unique_classes) < 2:
                print(f"Warning: Only one class found in targets: {unique_classes}")
                metrics['auroc'] = None
            else:
                metrics['auroc'] = roc_auc_score(y_true_binary, y_score_binary)
        except ValueError as e:
            print(f"Warning: Could not compute AUROC: {e}")
            metrics['auroc'] = None
    else:
        # Multi-class classification - compute AUROC for each class vs rest
        auroc_scores = []
        for i in range(num_classes):
            try:
                # One-vs-rest AUROC
                y_true_binary = (y_true == i).astype(int)
                y_score_binary = all_probabilities[:, i].astype(float)
                
                # Check if we have both classes for this class
                unique_classes = np.unique(y_true_binary)
                if len(unique_classes) < 2:
                    print(f"Warning: Only one class found for class {i}: {unique_classes}")
                    auroc_scores.append(None)
                else:
                    auroc_class = roc_auc_score(y_true_binary, y_score_binary)
                    auroc_scores.append(auroc_class)
            except ValueError as e:
                print(f"Warning: Could not compute AUROC for class {i}: {e}")
                auroc_scores.append(None)
        
        metrics['auroc_per_class'] = auroc_scores
        valid_scores = [score for score in auroc_scores if score is not None]
        metrics['auroc_mean'] = np.mean(valid_scores) if valid_scores else None
    
    return metrics, {
        'predictions': all_predictions,
        'probabilities': all_probabilities,
        'targets': all_targets,
        'file_paths': all_file_paths
    }


def evaluate_regression(model, dataloader, device):
    """Evaluate regression model and compute metrics."""
    model.to(device)
    model.eval()
    
    all_predictions = []
    all_targets = []
    all_file_paths = []
    
    with torch.no_grad():
        for batch in tqdm(dataloader, desc="Evaluating"):
            # Move batch to device
            batch = {k: v.to(device) if isinstance(v, torch.Tensor) else v for k, v in batch.items()}
            
            # Get inputs and targets
            inputs = batch["image"]
            targets = batch["label"]
            file_paths = batch["file_path"]
            
            # Forward pass
            outputs = model(inputs)
            predictions = outputs.squeeze()
            
            # Store results
            all_predictions.extend(predictions.cpu().numpy())
            all_targets.extend(targets.cpu().numpy())
            all_file_paths.extend(file_paths)
    
    # Convert to numpy arrays
    all_predictions = np.array(all_predictions)
    all_targets = np.array(all_targets)
    
    # Compute regression metrics
    mse = np.mean((all_predictions - all_targets) ** 2)
    rmse = np.sqrt(mse)
    mae = np.mean(np.abs(all_predictions - all_targets))
    
    # R-squared
    ss_res = np.sum((all_targets - all_predictions) ** 2)
    ss_tot = np.sum((all_targets - np.mean(all_targets)) ** 2)
    r2 = 1 - (ss_res / ss_tot) if ss_tot != 0 else 0
    
    metrics = {
        'mse': mse,
        'rmse': rmse,
        'mae': mae,
        'r2': r2
    }
    
    return metrics, {
        'predictions': all_predictions,
        'targets': all_targets,
        'file_paths': all_file_paths
    }


def main():
    logging.getLogger().setLevel(logging.INFO)
    
    parser = argparse.ArgumentParser(description="Test finetuned model and compute AUROC")
    parser.add_argument(
        "--checkpoint_path",
        type=str,
        required=True,
        help="Path to the model checkpoint (.ckpt file)"
    )
    parser.add_argument(
        "--data_dir",
        type=str,
        default="./data/preprocessed",
        help="Path to data directory"
    )
    parser.add_argument(
        "--output_dir",
        type=str,
        default="./test_results",
        help="Directory to save test results"
    )
    parser.add_argument(
        "--taskid",
        type=int,
        required=True,
        help="Task ID (1: FOMO1 classification, 2: FOMO2 classification, 3: FOMO3 regression, 4: PD classification)"
    )
    parser.add_argument(
        "--batch_size",
        type=int,
        default=4,
        help="Batch size for testing"
    )
    parser.add_argument(
        "--num_workers",
        type=int,
        default=4,
        help="Number of workers for data loading"
    )

    parser.add_argument(
        "--augmentation_preset",
        type=str,
        choices=["all", "basic", "none"],
        default="none",
        help="Augmentation preset for testing"
    )
    parser.add_argument(
        "--seed",
        type=int,
        default=42,
        help="Random seed for reproducibility"
    )
    parser.add_argument(
        "--model_name",
        type=str,
        default="unet_b",
        help="Model name (unet_b, unet_xl, etc.)"
    )
    parser.add_argument(
        "--patch_size",
        type=int,
        default=32,
        help="Patch size for the model"
    )
    
    args = parser.parse_args()
    
    # Set seed
    setup_seed(args.seed)
    
    # Get task configuration
    task_cfg = get_task_config(args.taskid)
    task_type = task_cfg["task_type"]
    task_name = task_cfg["task_name"]
    num_classes = task_cfg["num_classes"]
    modalities = len(task_cfg["modalities"])
    
    print(f"Testing model for task {args.taskid}: {task_name}")
    print(f"Task type: {task_type}")
    print(f"Number of classes: {num_classes}")
    
    # Set device
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"Using device: {device}")
    
    # Create output directory
    os.makedirs(args.output_dir, exist_ok=True)
    
    # Create test dataset
    print("Creating test dataset...")
    dataset, test_samples = create_test_dataset(
        data_dir=args.data_dir,
        task_name=task_name,
        patch_size=args.patch_size,
        task_type=task_type,
        augmentation_preset=args.augmentation_preset
    )
    
    print(f"Test dataset size: {len(dataset)}")
    
    # Create data loader
    dataloader = DataLoader(
        dataset,
        batch_size=args.batch_size,
        shuffle=False,
        num_workers=args.num_workers,
        pin_memory=True if device.type == "cuda" else False
    )
    
    # Load model
    print("Loading model from checkpoint...")
    config = {
        # Task information
        "task": task_name,
        "task_id": args.taskid,
        "task_type": task_type,
        "experiment": f"test_{task_name}",
        "model_name": args.model_name,
        "model_dimensions": "3D",
        "run_type": "test",
        # Directories
        "save_dir": args.output_dir,
        "train_data_dir": args.data_dir,
        "version_dir": args.output_dir,
        "version": 0,
        # Reproducibility
        "seed": args.seed,
        # Dataset properties
        "num_classes": num_classes,
        "num_modalities": modalities,
        "image_extension": ".npy",
        "allow_missing_modalities": False,
        "labels": task_cfg["labels"],
        # Training parameters (not used for testing but needed for model creation)
        "batch_size": args.batch_size,
        "learning_rate": 1e-4,  # Not used for testing
        "patch_size": (args.patch_size,) * 3,
        "precision": "bf16-mixed",
        "augmentation_preset": args.augmentation_preset,
        "epochs": 1,  # Not used for testing
        "train_batches_per_epoch": 1,  # Not used for testing
        "effective_batch_size": args.batch_size,
        # Dataset metrics
        "train_dataset_size": len(test_samples),
        "val_dataset_size": len(test_samples),
        "max_iterations": 1,  # Not used for testing
        # Hardware settings
        "num_devices": 1,
        "num_workers": args.num_workers,
        # Model compilation
        "compile": False,
        "compile_mode": "default",
        # Trainer specific params
        "fast_dev_run": False,
    }
    
    model = load_model_from_checkpoint(args.checkpoint_path, task_type, config)
    
    # Evaluate model
    print("Evaluating model...")
    if task_type == "regression":
        metrics, results = evaluate_regression(model, dataloader, device)
    else:
        metrics, results = evaluate_classification(model, dataloader, device, num_classes)
    
    # Print results
    print("\n" + "="*50)
    print("TEST RESULTS")
    print("="*50)
    for metric_name, metric_value in metrics.items():
        if metric_value is not None:
            print(f"{metric_name}: {metric_value:.4f}")
        else:
            print(f"{metric_name}: N/A")
    
    # Save results
    results_file = os.path.join(args.output_dir, f"test_results_task{args.taskid}.json")
    results_data = {
        "task_id": args.taskid,
        "task_name": task_name,
        "task_type": task_type,
        "metrics": metrics,
        "test_samples": test_samples,
        "config": {
            "checkpoint_path": args.checkpoint_path,
            "batch_size": args.batch_size,
        }
    }
    
    with open(results_file, 'w') as f:
        json.dump(results_data, f, indent=2, default=str)
    
    print(f"\nResults saved to: {results_file}")
    
    # Save detailed predictions
    predictions_file = os.path.join(args.output_dir, f"predictions_task{args.taskid}.npz")
    np.savez(
        predictions_file,
        predictions=results['predictions'],
        targets=results['targets'],
        file_paths=results['file_paths'],
        probabilities=results.get('probabilities', None)
    )
    
    print(f"Detailed predictions saved to: {predictions_file}")


if __name__ == "__main__":
    main() 

===============================================
File: augmentations/augmentation_composer.py
===============================================

from torchvision import transforms
from yucca.modules.data.augmentation.transforms.formatting import (
    AddBatchDimension,
    RemoveBatchDimension,
)
from yucca.modules.data.augmentation.transforms.BiasField import BiasField
from yucca.modules.data.augmentation.transforms.Blur import Blur
from yucca.modules.data.augmentation.transforms.copy_image_to_label import (
    CopyImageToLabel,
)
from yucca.modules.data.augmentation.transforms.Gamma import Gamma
from yucca.modules.data.augmentation.transforms.Ghosting import MotionGhosting
from yucca.modules.data.augmentation.transforms.Noise import (
    AdditiveNoise,
    MultiplicativeNoise,
)
from yucca.modules.data.augmentation.transforms.Ringing import GibbsRinging
from yucca.modules.data.augmentation.transforms.SimulateLowres import SimulateLowres
from yucca.modules.data.augmentation.transforms.Spatial import Spatial


def get_pretrain_augmentations(patch_size, preset):
    assert preset in ["none", "spatial", "all"]

    if preset == "none":
        augmentations = [CopyImageToLabel(copy=True)]

    elif preset == "spatial":
        augmentations = [spatial_augmentation(patch_size), CopyImageToLabel(copy=True)]

    elif preset == "all":
        augmentations = [
            spatial_augmentation(patch_size),
            CopyImageToLabel(copy=True),
        ] + intensity_augmentations()

    return transforms.Compose(
        [AddBatchDimension()] + augmentations + [RemoveBatchDimension()]
    )


def get_val_augmentations():
    return transforms.Compose(
        [AddBatchDimension(), CopyImageToLabel(copy=True), RemoveBatchDimension()]
    )


def get_finetune_augmentations(patch_size, preset):
    assert preset in ["none", "spatial", "all"]

    if preset == "none":
        return None

    elif preset == "spatial":
        augmentations = [spatial_augmentation(patch_size)]

    elif preset == "all":
        augmentations = [spatial_augmentation(patch_size)] + intensity_augmentations()

    return transforms.Compose(
        [AddBatchDimension()] + augmentations + [RemoveBatchDimension()]
    )


def spatial_augmentation(patch_size):
    return Spatial(
        patch_size=patch_size,
        crop=True,
        random_crop=False,
        cval="min",
        p_deform_per_sample=0.33,
        deform_sigma=(20, 30),
        deform_alpha=(200, 600),
        p_rot_per_sample=0.2,
        p_rot_per_axis=0.66,
        x_rot_in_degrees=(-30.0, 30.0),
        y_rot_in_degrees=(-30.0, 30.0),
        z_rot_in_degrees=(-30.0, 30.0),
        p_scale_per_sample=0.2,
        scale_factor=(0.9, 1.1),
        skip_label=True,
        clip_to_input_range=True,
    )


def intensity_augmentations():
    return [
        AdditiveNoise(
            p_per_sample=0.2,
            mean=(0.0, 0.0),
            sigma=(1e-3, 1e-4),
            clip_to_input_range=True,
        ),
        Blur(
            p_per_sample=0.2,
            p_per_channel=0.5,
            sigma=(0.0, 1.0),
            clip_to_input_range=True,
        ),
        MultiplicativeNoise(
            p_per_sample=0.2, mean=(0, 0), sigma=(1e-3, 1e-4), clip_to_input_range=True
        ),
        MotionGhosting(
            p_per_sample=0.2,
            alpha=(0.85, 0.95),
            num_reps=(2, 11),
            axes=(0, 3),
            clip_to_input_range=True,
        ),
        GibbsRinging(
            p_per_sample=0.2, cut_freq=(96, 129), axes=(0, 3), clip_to_input_range=True
        ),
        SimulateLowres(
            p_per_sample=0.2,
            p_per_channel=0.5,
            p_per_axis=0.33,
            zoom_range=(0.5, 1.0),
            clip_to_input_range=True,
        ),
        BiasField(p_per_sample=0.33, clip_to_input_range=True),
        Gamma(
            p_per_sample=0.2,
            p_invert_image=0.05,
            gamma_range=(0.5, 2.0),
            clip_to_input_range=True,
        ),
    ]


===============================================
File: augmentations/finetune_augmentation_presets.py
===============================================

from typing import Literal


def get_finetune_augmentation_params(preset: Literal["basic", "none", "yucca_default", "all"]) -> dict:
    """ "
    Get an augmentation parameter dict from a preset name.
    """
    if preset == "basic":
        return {
            # turned on
            "rotation_p_per_sample": 0.2,
            "rotation_p_per_axis": 0.66,
            "scale_p_per_sample": 0.2,
            "normalize": False,
            # turned off
            "additive_noise_p_per_sample": 0.0,
            "biasfield_p_per_sample": 0.0,
            "blurring_p_per_sample": 0.0,
            "blurring_p_per_channel": 0.0,
            "elastic_deform_p_per_sample": 0.0,
            "gamma_p_per_sample": 0.0,
            "gamma_p_invert_image": 0.0,
            "gibbs_ringing_p_per_sample": 0.0,
            "mirror_p_per_sample": 0.0,
            "mirror_p_per_axis": 0.0,
            "motion_ghosting_p_per_sample": 0.0,
            "multiplicative_noise_p_per_sample": 0.0,
            "simulate_lowres_p_per_sample": 0.0,
            "simulate_lowres_p_per_channel": 0.0,
            "simulate_lowres_p_per_axis": 0.0,
        }
    elif preset == "none":
        return {
            "rotation_p_per_sample": 0.0,
            "rotation_p_per_axis": 0.0,
            "scale_p_per_sample": 0.0,
            "additive_noise_p_per_sample": 0.0,
            "biasfield_p_per_sample": 0.0,
            "blurring_p_per_sample": 0.0,
            "blurring_p_per_channel": 0.0,
            "elastic_deform_p_per_sample": 0.0,
            "gamma_p_per_sample": 0.0,
            "gamma_p_invert_image": 0.0,
            "gibbs_ringing_p_per_sample": 0.0,
            "mirror_p_per_sample": 0.0,
            "mirror_p_per_axis": 0.0,
            "motion_ghosting_p_per_sample": 0.0,
            "multiplicative_noise_p_per_sample": 0.0,
            "simulate_lowres_p_per_sample": 0.0,
            "simulate_lowres_p_per_channel": 0.0,
            "simulate_lowres_p_per_axis": 0.0,
            "normalize": False,
        }
    elif preset == "all":
        return {"normalize": False}  # Will use Yucca defaults
    else:
        raise ValueError(f"Unknown augmentation preset: {preset}")


===============================================
File: augmentations/mask.py
===============================================

import torch
from utils.masking import generate_random_mask


def random_mask(x, mask_ratio, mask_patch_size, mask_token=0):
    mask = generate_random_mask(x, mask_ratio, mask_patch_size, out_type=bool)
    assert isinstance(mask, torch.BoolTensor) or isinstance(
        mask, torch.cuda.BoolTensor
    ), mask.type()
    x[mask] = mask_token
    return x, mask


===============================================
File: data/datamodule.py
===============================================

import lightning as pl
from torchvision.transforms import Compose
import logging
import torch
from typing import Literal, Optional, Tuple
from torch.utils.data import DataLoader, Sampler
from yucca.pipeline.configuration.split_data import SplitConfig
from yucca.functional.array_operations.matrix_ops import get_max_rotated_size
from yucca.modules.data.augmentation.transforms.Spatial import Spatial

from data.dataset import PretrainDataset


class PretrainDataModule(pl.LightningDataModule):
    def __init__(
        self,
        patch_size: Tuple[int, int, int],
        batch_size: int,
        num_workers: int,
        splits_config: SplitConfig,
        split_idx: int,
        train_data_dir: str,
        train_sampler: Optional[Sampler] = None,
        val_sampler: Optional[Sampler] = None,
        composed_train_transforms: Optional[Compose] = None,
        composed_val_transforms: Optional[Compose] = None,
    ):
        super().__init__()

        # extract parameters
        self.batch_size = batch_size
        self.patch_size = patch_size

        self.split_idx = split_idx
        self.splits_config = splits_config
        self.train_data_dir = train_data_dir

        self.composed_train_transforms = composed_train_transforms
        self.composed_val_transforms = composed_val_transforms
        self.pre_aug_patch_size = (
            get_max_rotated_size(patch_size)
            if augmentations_include_spatial(composed_train_transforms)
            else None
        )
        assert self.pre_aug_patch_size is None or isinstance(
            self.pre_aug_patch_size, tuple
        )

        self.num_workers = (
            max(0, int(torch.get_num_threads() - 1))
            if num_workers is None
            else num_workers
        )
        self.train_sampler = train_sampler
        self.val_sampler = val_sampler

        logging.info(f"Using {self.num_workers} workers")

    def setup(self, stage: Literal["fit", "test", "predict"]):
        assert stage == "fit"

        # Assign train/val datasets for use in dataloaders
        assert self.train_data_dir is not None
        assert self.split_idx is not None
        assert self.splits_config is not None

        self.train_samples = self.splits_config.train(self.split_idx)
        self.val_samples = self.splits_config.val(self.split_idx)

        self.train_dataset = PretrainDataset(
            self.train_samples,
            data_dir=self.train_data_dir,
            composed_transforms=self.composed_train_transforms,
            pre_aug_patch_size=self.pre_aug_patch_size,  # type: ignore
            patch_size=self.patch_size,
        )

        self.val_dataset = PretrainDataset(
            self.val_samples,
            data_dir=self.train_data_dir,
            composed_transforms=self.composed_val_transforms,
            patch_size=self.patch_size,
        )

    def train_dataloader(self):
        logging.info(f"Starting training with data from: {self.train_data_dir}")
        sampler = (
            self.train_sampler(self.train_dataset)
            if self.train_sampler is not None
            else None
        )

        return DataLoader(
            self.train_dataset,
            num_workers=self.num_workers,
            batch_size=self.batch_size,
            pin_memory=False,
            sampler=sampler,
            shuffle=sampler is None,
        )

    def val_dataloader(self):
        sampler = (
            self.val_sampler(self.val_dataset) if self.val_sampler is not None else None
        )

        return DataLoader(
            self.val_dataset,
            num_workers=self.num_workers,
            batch_size=self.batch_size,
            pin_memory=False,
            sampler=sampler,
        )


def augmentations_include_spatial(augmentations):
    if augmentations is None:
        return False

    for augmentation in augmentations.transforms:
        if isinstance(augmentation, Spatial):
            return True

    return False


===============================================
File: data/dataset.py
===============================================

from curses import meta
import torchvision
import numpy as np
import torch
import os
from torch.utils.data import Dataset
from typing import Tuple, Optional, Literal
from batchgenerators.utilities.file_and_folder_operations import load_pickle
from yucca.modules.data.augmentation.transforms.cropping_and_padding import CropPad
from yucca.modules.data.augmentation.transforms.formatting import NumpyToTorch

from batchgenerators.utilities.file_and_folder_operations import join


class FOMODataset(Dataset):
    """
    Dataset class for FOMO downstream tasks. Supports classification and regression tasks.
    For segmentation tasks, use YuccaTrainDataset from the Yucca library instead.
    """

    def __init__(
        self,
        samples: list,
        patch_size: Tuple[int, int, int],
        composed_transforms: Optional[torchvision.transforms.Compose] = None,
        task_type: Literal["classification", "regression"] = "classification",
        allow_missing_modalities: Optional[bool] = False,  # For compatibility
        p_oversample_foreground: Optional[float] = None,  # For compatibility
    ):
        super().__init__()
        # Support only non-segmentation tasks
        assert task_type in [
            "classification",
            "regression",
        ], f"Unsupported task type: {task_type}. For segmentation use YuccaTrainDataset instead."

        self.task_type = task_type
        self.all_files = samples
        self.composed_transforms = composed_transforms
        self.patch_size = patch_size

        self.croppad = CropPad(patch_size=self.patch_size)
        self.to_torch = NumpyToTorch()

    def __len__(self):
        return len(self.all_files)

    def __getitem__(self, idx):
        case = self.all_files[idx]

        # single modality
        assert isinstance(case, str)

        data = self._load_volume(case)
        label = self._load_label(case)
        data_dict = {
            "file_path": case,
            "image": data,
            "label": label,
        }

        metadata = {"foreground_locations": []}
        return self._transform(data_dict, metadata)

    def _transform(self, data_dict, metadata=None):
        label = data_dict["label"]
        data_dict["label"] = None
        data_dict = self.croppad(data_dict, metadata)
        if self.composed_transforms is not None:
            data_dict = self.composed_transforms(data_dict)

        data_dict["label"] = label
        return self.to_torch(data_dict)

    def _load_volume_and_header(self, file):
        vol = self._load_volume(file)
        header = load_pickle(file[: -len(".npy")] + ".pkl")
        return vol, header

    def _load_label(self, file):
        # For classification and regression, labels are in .txt files
        txt_file = file + ".txt"
        if self.task_type == "classification":
            return np.loadtxt(txt_file, dtype=int)
        else:  # regression
            reg_label = np.loadtxt(txt_file, dtype=float)
            reg_label = np.atleast_1d(reg_label)
            return reg_label

    def _load_volume(self, file):
        file = file + ".npy"

        try:
            vol = np.load(file, "r")
        except ValueError:
            vol = np.load(file, allow_pickle=True)

        return vol


class PretrainDataset(Dataset):
    def __init__(
        self,
        samples: list,
        patch_size: Tuple[int, int, int],
        data_dir: str,
        pre_aug_patch_size: Optional[Tuple[int, int, int]] = None,
        composed_transforms: Optional[torchvision.transforms.Compose] = None,
    ):
        self.all_files = samples
        self.data_dir = data_dir
        self.composed_transforms = composed_transforms
        self.patch_size = patch_size
        self.pre_aug_patch_size = pre_aug_patch_size

        self.croppad = CropPad(patch_size=self.pre_aug_patch_size or self.patch_size)
        self.to_torch = NumpyToTorch()

    def __len__(self):
        return len(self.all_files)

    def __getitem__(self, idx):
        case = self.all_files[idx]

        # single modality
        assert isinstance(case, str)
        data = self._load_volume(case)

        # Ensure volume does not contain NaNs or Infs, which can sometimes
        # occur in large pretraining datasets.
        if np.isnan(data).any() or np.isinf(data).any():
            if "DISABLE_NAN_WARNING" not in os.environ:
                print("A case contains NaNs or infs. We have corrected this, but consider handling this with different preprocessing or skipping affected cases.")
                print(f"Affected Case: {case}")
                print("Set DISABLE_NAN_WARNING=1 to disable this warning.")
            data = np.nan_to_num(data, nan=0.0, posinf=1.0, neginf=0.0, copy=True)

        data_dict = {
            "file_path": case
        }  # metadata that can be very useful for debugging.
        metadata = {"foreground_locations": []}
        data_dict["image"] = data

        return self._transform(data_dict, metadata)

    def _transform(self, data_dict, metadata=None):
        data_dict = self.croppad(data_dict, metadata)
        if self.composed_transforms is not None:
            data_dict = self.composed_transforms(data_dict)
        return self.to_torch(data_dict)

    def _load_volume_and_header(self, file):
        vol = self._load_volume(file)
        header = load_pickle(file[: -len(".npy")] + ".pkl")
        return vol, header

    def _load_volume(self, file):
        file = file + ".npy"
        path = join(self.data_dir, file)

        try:
            vol = np.load(path, "r")
        except ValueError:
            vol = np.load(path, allow_pickle=True)

        # Add channel dimension if it doesn't exist
        if len(vol.shape) == 3:
            vol = vol[np.newaxis, ...]

        return vol


===============================================
File: data/pretrain_split.py
===============================================

import logging
from batchgenerators.utilities.file_and_folder_operations import join, subfiles, isfile, save_pickle, load_pickle
from yucca.pipeline.configuration.configure_paths import PathConfig
from yucca.pipeline.configuration.split_data import SplitConfig, simple_split, split_is_precomputed, get_file_names


def get_pretrain_split_config(method: str, idx: int, split_ratio: float, path_config: PathConfig):
    splits_path = join(path_config.task_dir, "splits.pkl")

    assert method in [
        "simple_train_val_split",
        "multi_sequence_simple_train_val_split",
    ], "this module only supports a subset of the split methods"

    if isfile(splits_path):
        splits = load_pickle(splits_path)
        assert isinstance(splits, dict)

        if split_is_precomputed(splits, method, idx):
            logging.warning(
                f"Reusing already computed split file which was split using the {method} method and parameter {split_ratio}."
            )
            return SplitConfig(splits, method, idx)
        else:
            logging.warning("Generating new split since splits did not contain a split computed with the same parameters.")
    else:
        splits = {}

    if method not in splits.keys():
        splits[method] = {}

    assert method == "simple_train_val_split"
    names = get_file_names(path_config.train_data_dir)

    splits[method][split_ratio] = simple_split(names, split_ratio)  # type: ignore

    split_cfg = SplitConfig(splits, method, split_ratio)
    save_pickle(splits, splits_path)

    return split_cfg


def get_sequence_names(train_data_dir: str) -> list:
    filenames = subfiles(train_data_dir, join=False, suffix=".npy")
    groups = group_filenames(filenames)

    names = []

    # flatten the dict to list of dicts with metadata useful for debugging
    for dataset in groups.keys():
        for subject in groups[dataset].keys():
            for session in groups[dataset][subject].keys():
                names.append(
                    {
                        "dataset": dataset,
                        "subject": subject,
                        "session": session,
                        "filenames": groups[dataset][subject][session],
                    }
                )

    return names


===============================================
File: data/task_configs.py
===============================================

task1_config = {
    "task_name": "Task001_FOMO1",
    "crop_to_nonzero": True,
    "deep_supervision": False,
    "modalities": ("DWI", "T2FLAIR", "ADC", "SWI_OR_T2STAR"),
    "norm_op": "volume_wise_znorm",
    "num_classes": 2,
    "keep_aspect_ratio": True,
    "task_type": "classification",
    "label_extension": ".txt",
    "labels": {0: "Negative", 1: "Positive"},
}

task2_config = {
    "task_name": "Task002_FOMO2",
    "crop_to_nonzero": True,
    "deep_supervision": False,
    "modalities": ("DWI", "T2FLAIR", "SWI_OR_T2STAR"),
    "norm_op": "volume_wise_znorm",
    "num_classes": 2,
    "keep_aspect_ratio": True,
    "task_type": "segmentation",
    "label_extension": ".txt",
    "labels": {0: "background", 1: "menigioma"},
}

task3_config = {
    "task_name": "Task003_FOMO3",
    "crop_to_nonzero": True,
    "deep_supervision": False,
    "modalities": ("T1", "T2"),
    "norm_op": "volume_wise_znorm",
    "num_classes": 1,  # For regression, output dimension is 1
    "keep_aspect_ratio": True,
    "task_type": "regression",
    "label_extension": ".txt",
    "labels": {"regression": "Age"},  # Define as regression task
}

task4_config = {
    "task_name": "Task004_PD",
    "crop_to_nonzero": True,
    "deep_supervision": False,
    "modalities": ["T1"],
    "num_modalities": 1,
    "norm_op": "volume_wise_znorm",
    "num_classes": 2,
    "keep_aspect_ratio": True,
    "task_type": "classification",
    "label_extension": ".txt",
    "labels": {0: "Negative", 1: "Positive"},
}


===============================================
File: data/fomo-60k/preprocess.py
===============================================

import numpy as np
import argparse
import os
import multiprocessing as mp
from functools import partial
from batchgenerators.utilities.file_and_folder_operations import (
    join,
    save_pickle,
    maybe_mkdir_p as ensure_dir_exists,
)
from yucca.functional.preprocessing import preprocess_case_for_training_without_label
from yucca.functional.utils.loading import read_file_to_nifti_or_np
from utils.utils import parallel_process


def search_files_in_subdir(subdir):
    """
    Search for .nii.gz files in a single subdirectory.
    
    Args:
        subdir: Path to subdirectory to search
        
    Returns:
        list: List of (subject_name, session_name, scan_file, scan_path) tuples
    """
    scan_infos = []
    skip_dirs = {'.git', '__pycache__', '.DS_Store'}
    
    # Extract subject name from the subdir path
    subject_name = os.path.basename(subdir)
    
    try:
        for root, dirs, files in os.walk(subdir):
            # Skip unwanted directories
            dirs[:] = [d for d in dirs if d not in skip_dirs]
            
            # Only process files that end with .nii.gz AND contain "skull_stripped" in their name
            scan_files = [f for f in files if f.endswith(".nii.gz") and "skull_stripped" in f]
            
            for scan_file in scan_files:
                scan_path = os.path.join(root, scan_file)
                
                # Extract session name from the path
                # Path format: subdir/session_name/scan_file
                rel_path = os.path.relpath(scan_path, subdir)
                path_parts = rel_path.split(os.sep)
                
                if len(path_parts) >= 2:  # Ensure we have session/file structure
                    session_name = path_parts[0]
                    scan_infos.append((subject_name, session_name, scan_file, scan_path))
    except Exception as e:
        print(f"Error searching {subdir}: {e}")
    
    return scan_infos


def process_single_scan(scan_info, preprocess_config, target_dir, skip_existing=True):
    """
    Process a single scan for pretraining data.

    Args:
        scan_info: A tuple containing (subject_name, session_name, scan_file, scan_path)
        preprocess_config: Preprocessing configuration dictionary
        target_dir: Target directory for preprocessed data
        skip_existing: If True, skip processing if files already exist

    Returns:
        Success message or error message
    """
    subject_name, session_name, scan_file, scan_path = scan_info

    # Extract filename without extension to use as identifier
    scan_name = os.path.splitext(os.path.splitext(scan_file)[0])[0]

    # Create a unique filename for the preprocessed data
    filename = f"{subject_name}_{session_name}_{scan_name}"
    save_path = join(target_dir, filename)
    
    # Check if preprocessed files already exist
    if skip_existing and os.path.exists(save_path + ".npy") and os.path.exists(save_path + ".pkl"):
        print(f"Skipped {subject_name}/{session_name}/{scan_file} (already exists)")
        return

    # Check if file is a Git LFS pointer before processing
    try:
        with open(scan_path, 'rb') as f:
            header = f.read(100)  # Read first 100 bytes
            if header.startswith(b'version https://git-lfs.github.com/spec/v1'):
                print(f"Skipped {subject_name}/{session_name}/{scan_file} (Git LFS pointer - not downloaded)")
                return
    except Exception as e:
        print(f"Error checking file type for {subject_name}/{session_name}/{scan_file}: {str(e)}")
        return

    try:
        images, image_props = preprocess_case_for_training_without_label(
            images=[read_file_to_nifti_or_np(scan_path)], **preprocess_config
        )
        image = images[0]

        np.save(save_path + ".npy", image)
        save_pickle(image_props, save_path + ".pkl")

        print(f"Processed {subject_name}/{session_name}/{scan_file}")
    except Exception as e:
        print(f"Error processing {subject_name}/{session_name}/{scan_file}: {str(e)}")


def preprocess_pretrain_data(in_path: str, out_path: str, num_workers: int = None, skip_existing: bool = True):
    """
    Preprocess all pretraining data in parallel.

    Args:
        in_path: Path to the source data directory
        out_path: Path to store preprocessed data
        num_workers: Number of parallel workers (default: CPU count - 1)
        skip_existing: If True, skip processing if files already exist
    """
    target_dir = join(out_path, "FOMO60k")
    ensure_dir_exists(target_dir)

    preprocess_config = {
        "normalization_operation": ["volume_wise_znorm"],
        "crop_to_nonzero": True,
        "target_orientation": "RAS",
        "target_spacing": [1.0, 1.0, 1.0],
        "keep_aspect_ratio_when_using_target_size": False,
        "transpose": [0, 1, 2],
    }

    print(f"Preprocessing configuration: {preprocess_config}")
    print(f"Target directory: {target_dir}")
    print(f"Skip existing: {skip_existing}")

    # Get all immediate subdirectories (subjects) for parallel search
    subdirs = [os.path.join(in_path, d) for d in os.listdir(in_path) 
               if os.path.isdir(os.path.join(in_path, d))]
    
    print(f"Found {len(subdirs)} subdirectories to search")
    print(f"Using {num_workers} parallel workers for file discovery")
    
    # Search for files in parallel
    if num_workers is None:
        num_workers = max(1, mp.cpu_count() - 1)
    
    with mp.Pool(processes=num_workers) as pool:
        results = pool.map(search_files_in_subdir, subdirs)
    
    # Combine results from all workers
    scan_infos = []
    for result in results:
        scan_infos.extend(result)
    
    print(f"Found {len(scan_infos)} scans to process")
    
    # Sort for consistent processing order
    scan_infos.sort()

    # Create partial function with fixed arguments
    process_func = partial(
        process_single_scan, preprocess_config=preprocess_config, target_dir=target_dir, skip_existing=skip_existing
    )

    # Process all scans in parallel using the shared utility function
    print(f"Processing {len(scan_infos)} scans in parallel using {num_workers} workers")
    parallel_process(process_func, scan_infos, num_workers, desc="Preprocessing scans")
    print(f"Preprocessing completed!")


if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument(
        "--in_path", type=str, required=True, help="Path to pretrain data"
    )
    parser.add_argument(
        "--out_path",
        type=str,
        required=True,
        help="Path to put preprocessed pretrain data",
    )
    parser.add_argument(
        "--num_workers",
        type=int,
        default=None,
        help="Number of parallel workers to use. Default is CPU count - 1",
    )
    parser.add_argument(
        "--no_skip_existing",
        action="store_true",
        help="If set, reprocess files even if they already exist (default: skip existing files)",
    )
    args = parser.parse_args()
    preprocess_pretrain_data(
        in_path=args.in_path, out_path=args.out_path, num_workers=args.num_workers, skip_existing=not args.no_skip_existing
    )

===============================================
File: data/preprocess/fomo1.py
===============================================

import os
import shutil
import numpy as np
import nibabel as nib
from batchgenerators.utilities.file_and_folder_operations import (
    join,
    maybe_mkdir_p as ensure_dir_exists,
)
from yucca.functional.preprocessing import preprocess_case_for_training_without_label
from data.task_configs import task1_config
from utils.utils import parallel_process


def process_subject(task_info):
    """
    Process a single subject for Task 1.

    Args:
        task_info: A tuple containing (folder_name, source_path, labels_dir, pp_config, target_preprocessed, prefix)

    Returns:
        Success message or error message
    """
    folder_name, source_path, labels_dir, pp_config, target_preprocessed, prefix = (
        task_info
    )
    modalities = pp_config["modalities"]

    try:
        images_dir = join(source_path, "preprocessed")
        session_path = join(images_dir, folder_name, "ses_1")

        if not os.path.isdir(session_path):
            return f"Error: {folder_name} is not a valid directory"

        # Get label file
        label_file = join(labels_dir, folder_name, "ses_1", "label.txt")
        if not os.path.exists(label_file):
            return f"Error: No label file found for {folder_name}"

        subject_id = folder_name.replace(".", "_")

        # Collect images for all modalities
        image_files = []
        modality_mapping = {}

        for file in os.listdir(session_path):
            if not file.endswith(".nii.gz"):
                continue

            # Determine modality
            if "dwi" in file:
                modality_index = 0  # DWI
            elif "flair" in file:
                modality_index = 1  # T2FLAIR
            elif "adc" in file:
                modality_index = 2  # ADC
            elif "swi" in file or "t2s" in file:
                modality_index = 3  # SWI_OR_T2STAR
            else:
                continue

            source_img = join(session_path, file)
            image_files.append(source_img)
            modality_mapping[modality_index] = source_img

        # Skip if we don't have all required modalities
        if len(image_files) < len(modalities):
            return f"Error: Not all modalities found for {folder_name}"

        # Load and preprocess images
        images = [
            nib.load(modality_mapping[i])
            for i in range(len(modalities))
            if i in modality_mapping
        ]

        # Apply preprocessing
        preprocessed_images, _ = preprocess_case_for_training_without_label(
            images=images,
            normalization_operation=[
                pp_config["norm_op"] for _ in pp_config["modalities"]
            ],
            allow_missing_modalities=False,
            crop_to_nonzero=pp_config["crop_to_nonzero"],
        )

        # Save preprocessed data
        save_path = join(target_preprocessed, f"{prefix}_{subject_id}")
        np.save(save_path + ".npy", preprocessed_images)
        shutil.copy(label_file, join(target_preprocessed, f"{prefix}_{subject_id}.txt"))

        return f"Processed {folder_name}"

    except Exception as e:
        return f"Error processing {folder_name}: {str(e)}"


def convert_and_preprocess_task1(
    source_path: str,
    output_path: str,
    num_workers=None,
):
    """
    Preprocess all subjects for Task 1 in parallel.

    Args:
        source_path: Path to the source data directory
        output_path: Path where preprocessed data will be saved
        num_workers: Number of parallel workers (default: CPU count - 1)
    """
    # Get configuration from task1_config
    pp_config = task1_config
    task_name = pp_config["task_name"]
    prefix = "FOMO1"

    # Input data paths
    labels_dir = join(source_path, "labels")
    images_dir = join(source_path, "preprocessed")

    # Output path for preprocessed data
    target_preprocessed = join(output_path, task_name)

    # Create directory
    ensure_dir_exists(target_preprocessed)

    # Collect all subjects to process
    folder_names = [
        f
        for f in os.listdir(images_dir)
        if os.path.isdir(join(images_dir, f, "ses_1"))
    ]

    assert len(folder_names) > 0, "Did not collect any subjects to preprocess."

    # Create task information for each subject
    tasks = [
        (folder_name, source_path, labels_dir, pp_config, target_preprocessed, prefix)
        for folder_name in folder_names
    ]

    # Process all subjects in parallel
    parallel_process(
        process_subject, tasks, num_workers, desc="Processing subjects for Task 1"
    )

    print(f"Task 1 preprocessing completed. Data saved to {target_preprocessed}")


===============================================
File: data/preprocess/fomo2.py
===============================================

import os
import numpy as np
import nibabel as nib
from batchgenerators.utilities.file_and_folder_operations import (
    join,
    maybe_mkdir_p as ensure_dir_exists,
    save_pickle,
)
from yucca.functional.preprocessing import preprocess_case_for_training_with_label
from data.task_configs import task2_config
from utils.utils import parallel_process


def process_subject(task_info):
    """
    Process a single subject for Task 2.

    Args:
        task_info: A tuple containing (subject, source_path, images_dir, labels_dir, pp_config, target_preprocessed, prefix)

    Returns:
        Success message or error message
    """
    (
        subject,
        source_path,
        images_dir,
        labels_dir,
        pp_config,
        target_preprocessed,
        prefix,
    ) = task_info
    modalities = pp_config["modalities"]

    try:
        session_path = join(images_dir, subject, "ses_1")
        label_path = join(labels_dir, subject, "ses_1", "seg.nii.gz")

        if not os.path.exists(session_path) or not os.path.exists(label_path):
            return f"Error: Missing data for {subject}"

        # Get all modality images
        image_files = []
        modality_mapping = {}

        for file in os.listdir(session_path):
            if not file.endswith(".nii.gz"):
                continue

            # Determine modality
            if "dwi" in file.lower():
                modality_index = 0  # DWI
            elif "flair" in file.lower():
                modality_index = 1  # T2FLAIR
            elif "swi" in file.lower() or "t2s" in file.lower():
                modality_index = 2  # SWI_OR_T2STAR
            else:
                return f"Warning: Skipping file {file}"

            source_img = join(session_path, file)
            image_files.append(source_img)
            modality_mapping[modality_index] = source_img

        # Skip if we don't have all required modalities
        if len(image_files) < len(modalities):
            return f"Error: Not all modalities found for {subject}"

        # Load images for preprocessing
        images = [
            nib.load(modality_mapping[i])
            for i in range(len(modalities))
            if i in modality_mapping
        ]

        # Load segmentation label
        label = nib.load(label_path)

        # Apply preprocessing with label
        preprocessed_data, preprocessed_label, properties = (
            preprocess_case_for_training_with_label(
                images=images,
                label=label,
                normalization_operation=[
                    pp_config["norm_op"] for _ in pp_config["modalities"]
                ],
                allow_missing_modalities=False,
                crop_to_nonzero=pp_config["crop_to_nonzero"],
                keep_aspect_ratio_when_using_target_size=pp_config["keep_aspect_ratio"],
            )
        )

        data_with_label = preprocessed_data + [preprocessed_label]

        # Save preprocessed data
        save_path = join(target_preprocessed, f"{prefix}_{subject}")
        np.save(save_path + ".npy", np.array(data_with_label, dtype=object))

        # Save properties as pickle
        save_pickle(properties, save_path + ".pkl")

        return f"Processed {subject}"

    except Exception as e:
        return f"Error processing {subject}: {str(e)}"


def convert_and_preprocess_task2(
    source_path: str,
    output_path: str,
    num_workers=None,
):
    """
    Preprocess all subjects for Task 2 in parallel.

    Args:
        source_path: Path to the source data directory
        output_path: Path where preprocessed data will be saved (optional)
        num_workers: Number of parallel workers (default: CPU count - 1)
    """
    # Get configuration from task2_config
    pp_config = task2_config
    task_name = pp_config["task_name"]
    prefix = "FOMO2"

    # Input data paths
    labels_dir = join(source_path, "labels")
    images_dir = join(source_path, "preprocessed")

    # Output path for preprocessed data
    target_preprocessed = join(output_path, task_name)

    # Create directory
    ensure_dir_exists(target_preprocessed)

    # Collect subjects to process
    subjects = sorted(os.listdir(images_dir))

    # Create task information for each subject
    tasks = [
        (
            subject,
            source_path,
            images_dir,
            labels_dir,
            pp_config,
            target_preprocessed,
            prefix,
        )
        for subject in subjects
    ]

    # Process all subjects in parallel
    parallel_process(
        process_subject, tasks, num_workers, desc="Processing subjects for Task 2"
    )

    print(f"Task 2 preprocessing completed. Data saved to {target_preprocessed}")


===============================================
File: data/preprocess/fomo3.py
===============================================

import os
import numpy as np
import nibabel as nib
from batchgenerators.utilities.file_and_folder_operations import (
    join,
    maybe_mkdir_p as ensure_dir_exists,
)
from yucca.functional.preprocessing import preprocess_case_for_training_without_label
from data.task_configs import task3_config
from utils.utils import parallel_process


def process_subject(task_info):
    """
    Process a single subject for Task 3.

    Args:
        task_info: A tuple containing (subject, source_path, labels_dir, pp_config, target_preprocessed, prefix)

    Returns:
        Success message or error message
    """
    subject, source_path, labels_dir, pp_config, target_preprocessed, prefix = task_info

    try:
        subject_id = subject.replace("sub_", "")
        session_path = join(source_path, "preprocessed", subject, "ses_1")

        if not os.path.isdir(session_path):
            return f"Error: {session_path} is not a valid directory"

        # Get label file
        label_file = join(labels_dir, subject, "ses_1", "label.txt")
        if not os.path.exists(label_file):
            return f"Error: No label file found for {subject}"

        # Read the age from the label file - keep as continuous value for regression
        with open(label_file, "r") as f:
            try:
                age = float(f.read().strip())
            except ValueError:
                return f"Error: Invalid age format in {label_file}"

        # Get all image files in the session directory
        image_files = [f for f in os.listdir(session_path) if f.endswith(".nii.gz")]

        # There should be two MR images per subject
        if len(image_files) != 2:
            return f"Warning: Expected 2 images for {subject}, but found {len(image_files)}. Still processing."

        # Sort images to ensure consistent ordering
        sorted_image_files = sorted(image_files)

        # Load images for preprocessing
        images = []
        for image_file in sorted_image_files:
            source_img = join(session_path, image_file)
            # Load image
            img = nib.load(source_img)
            images.append(img)

        # Apply preprocessing
        preprocessed_images, _ = preprocess_case_for_training_without_label(
            images=images,
            normalization_operation=[
                pp_config["norm_op"] for _ in pp_config["modalities"]
            ],
            allow_missing_modalities=False,
            crop_to_nonzero=pp_config["crop_to_nonzero"],
        )

        # Save preprocessed data
        save_path = join(target_preprocessed, f"{prefix}_{subject_id}")
        np.save(save_path + ".npy", preprocessed_images)

        # Save label for preprocessed data
        with open(join(target_preprocessed, f"{prefix}_{subject_id}.txt"), "w") as f:
            f.write(str(age))

        return f"Processed {subject}"

    except Exception as e:
        return f"Error processing {subject}: {str(e)}"


def convert_and_preprocess_task3(
    source_path: str,
    output_path: str,
    num_workers=None,
):
    """
    Preprocess all subjects for Task 3 in parallel.

    Args:
        source_path: Path to the source data directory
        output_path: Path where preprocessed data will be saved (optional)
        num_workers: Number of parallel workers (default: CPU count - 1)
    """
    # Get configuration from task3_config
    pp_config = task3_config
    task_name = pp_config["task_name"]
    prefix = "FOMO3"

    # Input data paths
    labels_dir = join(source_path, "labels")
    images_dir = join(source_path, "preprocessed")

    # Output path for preprocessed data
    target_preprocessed = join(output_path, task_name)

    # Create directory
    ensure_dir_exists(target_preprocessed)

    # Process all subjects
    subject_folders = [f for f in os.listdir(images_dir) if f.startswith("sub_")]

    # Create task information for each subject
    tasks = [
        (subject, source_path, labels_dir, pp_config, target_preprocessed, prefix)
        for subject in subject_folders
    ]

    # Process all subjects in parallel
    parallel_process(
        process_subject, tasks, num_workers, desc="Processing subjects for Task 3"
    )

    print(f"Task 3 preprocessing completed. Data saved to {target_preprocessed}")


===============================================
File: data/preprocess/run_preprocessing.py
===============================================

import argparse

from data.preprocess.fomo1 import convert_and_preprocess_task1
from data.preprocess.fomo2 import convert_and_preprocess_task2
from data.preprocess.fomo3 import convert_and_preprocess_task3


def preprocess_task(
    taskid: int,
    source_path: str,
    output_path: str,
    num_workers: int = None,
):
    """
    Preprocess the specified task using parallel processing.

    Args:
        taskid: Task ID to preprocess (1, 2, or 3)
        source_path: Path to the source data directory
        output_path: Path where preprocessed data will be saved (optional)
        num_workers: Number of parallel workers to use (default: CPU count - 1)
    """
    assert taskid in [1, 2, 3], f"Task {taskid} not supported"

    print(f"Starting preprocessing for Task {taskid}")

    if taskid == 1:
        convert_and_preprocess_task1(
            source_path=source_path, output_path=output_path, num_workers=num_workers
        )
    elif taskid == 2:
        convert_and_preprocess_task2(
            source_path=source_path, output_path=output_path, num_workers=num_workers
        )
    elif taskid == 3:
        convert_and_preprocess_task3(
            source_path=source_path, output_path=output_path, num_workers=num_workers
        )


if __name__ == "__main__":
    parser = argparse.ArgumentParser(
        description="Parallelize preprocessing for FOMO25 Challenge tasks"
    )
    parser.add_argument(
        "--taskid", type=int, required=True, help="Task ID to preprocess (1, 2, or 3)"
    )
    parser.add_argument(
        "--source_path", type=str, required=True, help="Path to the source data directory"
    )
    parser.add_argument(
        "--output_path",
        type=str,
        default="data/preprocessed",
        help="Path to save preprocessed data (default: data/preprocessed)",
    )
    parser.add_argument(
        "--num_workers",
        type=int,
        default=None,
        help="Number of parallel workers to use for preprocessing. Default is CPU count - 1",
    )
    args = parser.parse_args()

    preprocess_task(args.taskid, args.source_path, args.output_path, args.num_workers)


===============================================
File: inference/apptainer_template.def
===============================================

Bootstrap: docker

# Use any docker image as a base (see https://hub.docker.com/)
# If using GPU, consider using a CUDA-enabled base image
From: python:3.11-slim

%labels
    Author FOMO25 Challenge
    Version v1.0.0
    Description FOMO25 Apptainer Image Template

%environment
    export PYTHONUNBUFFERED=1
    export LC_ALL=C.UTF-8

%files
    # Copy your files (model, predict.py, requirements.txt, ...) to the container
    ./src/* /app/
    ./src/inference/container_requirements.txt /app/requirements.txt

    # ----> CHANGES REQUIRED <---
    ./src/inference/predict_task2.py /app/predict.py  # <-- TODO: Change to correct task
    /path/to/checkpoint.ckpt /app/model.ckpt # <--- TODO: Change to correct path

%post
    # Create necessary directories
    mkdir -p /input /output /app

    # Update and install system dependencies
    apt-get update && apt-get install -y --no-install-recommends \
        build-essential \
        && rm -rf /var/lib/apt/lists/*

    # Install Python dependencies
    pip install --no-cache-dir -U pip setuptools wheel
    pip install --no-cache-dir -r /app/requirements.txt

    # Make predict.py executable
    chmod +x /app/predict.py

%runscript
    # Execute the prediction script with all arguments
    exec python /app/predict.py "$@"


===============================================
File: inference/container_requirements.txt
===============================================

lightning>=2.1.4
numpy>=1.23,<2.0
yucca==2.2.6
opencv-python>=4.8.1.78
einops>=0.7
torch<2.3.0
nnunetv2>=2.4.2
wandb>=0.16.0
opencv-python>=4.8.0


===============================================
File: inference/predict_task1.py
===============================================

#!/usr/bin/env python
import argparse
import torch
from torch.nn.functional import softmax
from data.task_configs import task1_config
from inference.predict import predict_from_config, save_output_txt

# Task-specific hardcoded configuration
predict_config = {
    # Import values from task_configs
    **task1_config,
    # Add inference-specific configs
    "model_path": "/app/model.ckpt",  # Path to model (inside container!)
    "patch_size": (96, 96, 96),  # Patch size for inference
}


def main():
    parser = argparse.ArgumentParser(
        description="Run inference on FOMO Task 1 (Infarct Detection)"
    )

    # Input and output paths using modality names from task config
    parser.add_argument(
        "--dwi_b1000", type=str, required=True, help="Path to DWI image (NIfTI format)"
    )
    parser.add_argument(
        "--flair",
        type=str,
        required=True,
        help="Path to T2FLAIR image (NIfTI format)",
    )
    parser.add_argument(
        "--adc", type=str, required=True, help="Path to ADC image (NIfTI format)"
    )
    parser.add_argument(
        "--swi", type=str, required=False, help="Path to SWI image (NIfTI format)"
    )
    parser.add_argument(
        "--t2s", type=str, required=False, help="Path to T2* image (NIfTI format)"
    )
    parser.add_argument(
        "--output", type=str, required=True, help="Output path for prediction"
    )

    # Parse arguments
    args = parser.parse_args()

    assert (args.swi and not args.t2s) or (
        not args.swi and args.t2s
    ), "Either --swi or --t2s must be provided, but not both."

    # Map arguments to modality paths in expected order from task config
    modality_paths = [args.dwi_b1000, args.flair, args.adc, args.swi or args.t2s]
    output_path = args.output

    # Run prediction using the shared prediction logic
    predictions_original, _ = predict_from_config(
        modality_paths=modality_paths,
        predict_config=predict_config,
    )

    # softmax output to get probability
    probabilities = softmax(predictions_original, dim=1)

    save_output_txt(probabilities[0][1], output_path)


if __name__ == "__main__":
    main()


===============================================
File: inference/predict_task2.py
===============================================

#!/usr/bin/env python

import argparse
import numpy as np

from data.task_configs import task2_config
from inference.predict import predict_from_config, save_segmentation

# Task-specific hardcoded configuration
predict_config = {
    # Import values from task_configs
    **task2_config,
    # Add inference-specific configs
    "model_path": "/app/model.ckpt",  # Path to model (inside container!)
    "patch_size": (96, 96, 96),
}


def main():
    parser = argparse.ArgumentParser(
        description="Run inference on FOMO Task 2 (Meningioma Segmentation)"
    )

    # Input and output paths using modality names from task config
    parser.add_argument(
        "--dwi_b1000", type=str, required=True, help="Path to DWI image (NIfTI format)"
    )
    parser.add_argument(
        "--flair",
        type=str,
        required=True,
        help="Path to T2FLAIR image (NIfTI format)",
    )
    parser.add_argument(
        "--swi", type=str, required=False, help="Path to SWI image (NIfTI format)"
    )
    parser.add_argument(
        "--t2s", type=str, required=False, help="Path to T2* image (NIfTI format)"
    )
    parser.add_argument(
        "--output", type=str, required=True, help="Output path for prediction"
    )

    # Parse arguments
    args = parser.parse_args()

    assert (args.swi and not args.t2s) or (
        not args.swi and args.t2s
    ), "Either --swi or --t2s must be provided, but not both."

    # Map arguments to modality paths in expected order from task config
    modality_paths = [args.dwi_b1000, args.flair, args.swi or args.t2s]
    output_path = args.output

    # Run prediction using the shared prediction logic
    predictions, affine = predict_from_config(
        modality_paths=modality_paths,
        predict_config=predict_config,
        reverse_preprocess=True,
    )

    prediction_final = np.argmax(predictions[0], axis=0).astype(np.int32)

    save_segmentation(prediction_final, affine, output_path)


if __name__ == "__main__":
    main()


===============================================
File: inference/predict_task3.py
===============================================

#!/usr/bin/env python

import argparse

from data.task_configs import task3_config
from inference.predict import predict_from_config, save_output_txt

# Task-specific hardcoded configuration
predict_config = {
    # Import values from task_configs
    **task3_config,
    # Add inference-specific configs
    "model_path": "/app/model.ckpt",  # Path to model (inside container!)
    "patch_size": (96, 96, 96),
}


def main():
    parser = argparse.ArgumentParser(
        description="Run inference on FOMO Task 3 (Brain Age Regression)"
    )

    # Input and output paths using modality names from task config
    parser.add_argument(
        "--t1", type=str, required=True, help="Path to T1 image (NIfTI format)"
    )
    parser.add_argument(
        "--t2", type=str, required=True, help="Path to T2 image (NIfTI format)"
    )
    parser.add_argument(
        "--output", type=str, required=True, help="Output path for prediction"
    )

    # Parse arguments
    args = parser.parse_args()

    # Map arguments to modality paths in expected order from task config
    modality_paths = [args.t1, args.t2]
    output_path = args.output

    # Run prediction using the shared prediction logic
    prediction, _ = predict_from_config(
        modality_paths=modality_paths,
        predict_config=predict_config,
    )

    save_output_txt(int(prediction[0, 0]), output_path)


if __name__ == "__main__":
    main()


===============================================
File: inference/predict.py
===============================================

#!/usr/bin/env python

import os
import torch
import numpy as np
import nibabel as nib
from typing import List, Dict, Any
from models.supervised_seg import SupervisedSegModel
from models.supervised_cls import SupervisedClsModel
from models.supervised_reg import SupervisedRegModel
from data.task_configs import task1_config, task2_config, task3_config

from yucca.functional.preprocessing import (
    preprocess_case_for_inference,
    reverse_preprocessing,
)


def get_task_config(taskid):
    """Get task configuration based on task ID."""
    if taskid == 1:
        task_cfg = task1_config
    elif taskid == 2:
        task_cfg = task2_config
    elif taskid == 3:
        task_cfg = task3_config
    else:
        raise ValueError(f"Unknown taskid: {taskid}. Supported IDs are 1, 2, and 3")

    return task_cfg


def load_modalities(modality_paths: List[str]) -> List[nib.Nifti1Image]:
    """Load modality images from provided paths."""
    images = []
    for path in modality_paths:
        if not os.path.exists(path):
            raise FileNotFoundError(f"Modality file not found: {path}")
        try:
            img = nib.load(path)
            images.append(img)
        except Exception as e:
            raise RuntimeError(f"Failed to load image {path}: {str(e)}")

    return images


def save_segmentation(
    prediction: np.ndarray, affine: nib.Nifti1Image, output_path: str
):
    """Save prediction as a NIfTI file using affine from reference image."""
    # Create a new NIfTI image with the prediction data and reference affine
    pred_nifti = nib.Nifti1Image(prediction, affine)

    # Ensure output directory exists
    os.makedirs(os.path.dirname(os.path.abspath(output_path)), exist_ok=True)

    # Make sure output path has .nii.gz extension
    if not output_path.endswith((".nii", ".nii.gz")):
        output_path = output_path + ".nii.gz"

    # Save the prediction
    nib.save(pred_nifti, output_path)


def save_output_txt(number: float | int, output_path: str):
    """Save a number (float or int) as plain text to a file."""

    os.makedirs(os.path.dirname(os.path.abspath(output_path)), exist_ok=True)

    if not output_path.endswith(".txt"):
        output_path = output_path + ".txt"

    with open(output_path, "w") as f:
        f.write(f"{number}")


def predict_from_config(
    modality_paths: List[str],
    predict_config: Dict[str, Any],
    reverse_preprocess: bool = False,
):
    """
    Run inference on input modality images using a task-specific configuration.

    Args:
        modality_paths: Paths to input modality images
        predict_config: Dictionary containing all the configuration parameters for prediction

    Returns:
        str: Path to saved prediction
    """
    # Load input images
    images = load_modalities(modality_paths)

    # Extract configuration parameters
    task_type = predict_config["task_type"]
    crop_to_nonzero = predict_config["crop_to_nonzero"]
    norm_op = predict_config["norm_op"]
    num_classes = predict_config["num_classes"]
    keep_aspect_ratio = predict_config.get("keep_aspect_ratio", True)
    patch_size = predict_config["patch_size"]
    model_path = predict_config["model_path"]

    # Define preprocessing parameters
    normalization_scheme = [norm_op] * len(modality_paths)
    target_spacing = [1.0, 1.0, 1.0]  # Isotropic 1mm spacing
    target_orientation = "RAS"

    # Apply preprocessing
    case_preprocessed, case_properties = preprocess_case_for_inference(
        crop_to_nonzero=crop_to_nonzero,
        images=images,
        intensities=None,  # Use default intensity normalization
        normalization_scheme=normalization_scheme,
        patch_size=patch_size,
        target_size=None,  # We use target_spacing instead
        target_spacing=target_spacing,
        target_orientation=target_orientation,
        allow_missing_modalities=False,
        keep_aspect_ratio=keep_aspect_ratio,
        transpose_forward=[0, 1, 2],  # Standard transpose order
    )

    # Load the model checkpoint directly with Lightning
    if task_type == "segmentation":
        model = SupervisedSegModel.load_from_checkpoint(checkpoint_path=model_path)
    elif task_type == "classification":
        model = SupervisedClsModel.load_from_checkpoint(checkpoint_path=model_path)
    elif task_type == "regression":
        model = SupervisedRegModel.load_from_checkpoint(checkpoint_path=model_path)
    else:
        raise ValueError(f"Unsupported task type: {task_type}")

    # Set model to evaluation mode
    model.eval()

    # Get device
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model = model.to(device)
    case_preprocessed = case_preprocessed.to(device)

    # Run inference
    with torch.no_grad():
        # Set up sliding window parameters
        overlap = 0.5  # Standard overlap for sliding window

        # Get prediction
        predictions = model.model.predict(
            data=case_preprocessed,
            mode="3D",
            mirror=False,  # No test-time augmentation
            overlap=overlap,
            patch_size=patch_size,
            sliding_window_prediction=task_type == "segmentation",
        )

    if reverse_preprocess:
        predictions_original, _ = reverse_preprocessing(
            crop_to_nonzero=crop_to_nonzero,
            images=predictions,
            image_properties=case_properties,
            n_classes=num_classes,
            transpose_forward=[0, 1, 2],
            transpose_backward=[0, 1, 2],
        )
        print(f"-- Prediction shape: {predictions_original.shape}")
        return predictions_original, images[0].affine
    else:
        print(f"-- Prediction shape: {predictions.shape}")
        return predictions, None


===============================================
File: models/__init__.py
===============================================



===============================================
File: models/self_supervised.py
===============================================

import copy
from typing import List
import lightning as L
import torch
import torch.nn as nn
from torch.optim import Adam, AdamW
from torch.optim.lr_scheduler import LinearLR, CosineAnnealingLR, SequentialLR

from yucca.functional.utils.kwargs import filter_kwargs

from augmentations.mask import random_mask
import utils.visualisation as viz
from models import networks
import warnings
from lightning.pytorch.loggers import WandbLogger


class SelfSupervisedModel(L.LightningModule):
    def __init__(
        self,
        model_name: str,
        steps_per_epoch: int,
        epochs: int,
        learning_rate: float,
        config: dict,
        optimizer: str = "AdamW",
        warmup_epochs: int = 10,
        cosine_period_ratio: float = 1,
        input_channels: int = 1,
        num_classes: int = 1,
        patch_size: list | tuple = None,
        mask_patch_size: int = 4,
        mask_ratio: float = 0.6,
        should_compile: bool = False,
        compile_mode: str = None,
        debug_losses: bool = False,
        rec_loss_masked_only: bool = False,
        disable_image_logging: bool = False,
    ):
        super().__init__()
        # Model parameters
        self.num_classes = num_classes
        self.input_channels = input_channels
        self.model_name = model_name
        self.patch_size = patch_size

        self.version_dir = config["version_dir"]
        self.batch_size = config["batch_size"]

        # Loss, optimizer and scheduler parameters
        self.learning_rate = learning_rate

        # we might want to log the reconstruction loss on a per dataset or per modality level
        # in which case we do not want to reduce the loss before this has been done
        self.debug_losses = debug_losses and self.reconstruction
        mse_reduction = "none" if self.debug_losses else "mean"

        # losses
        self._rec_loss_fn = nn.MSELoss(reduction=mse_reduction)  # reconstruction
        self.rec_loss_masked_only = rec_loss_masked_only
        self.optimizer = optimizer

        self.steps_per_epoch = steps_per_epoch
        self.epochs = epochs
        self.warmup_epochs = warmup_epochs
        self.cosine_period_ratio = cosine_period_ratio
        assert 0 < cosine_period_ratio <= 1

        self.mask_ratio = mask_ratio
        self.mask_patch_size = mask_patch_size

        self.should_compile = should_compile
        self.compile_mode = compile_mode

        self.disable_image_logging = disable_image_logging

        print(
            f"Compile settings are should_compile: {should_compile}, compile_mode: {compile_mode}"
        )

        # Save params and start training
        self.save_hyperparameters()
        self.load_model()

    def load_model(self):
        print(f"Loading Model: 3D {self.model_name}")
        model_func = getattr(networks, self.model_name)

        print("Found model: ", model_func)

        conv_op = torch.nn.Conv3d
        norm_op = torch.nn.InstanceNorm3d

        model_kwargs = {
            # Applies to all models
            "input_channels": self.input_channels,
            "num_classes": self.num_classes,
            # Applies to most CNN-based architectures
            "conv_op": conv_op,
            # Applies to most CNN-based architectures (exceptions: UXNet)
            "norm_op": norm_op,
            # Pretrainnig
            "mode": "mae",
            "patch_size": self.patch_size,
        }
        model_kwargs = filter_kwargs(model_func, model_kwargs)
        model = model_func(**model_kwargs)

        self.model = (
            torch.compile(model, mode=self.compile_mode)
            if self.should_compile
            else model
        )

    def training_step(self, batch, batch_idx):
        x, y = batch["image"], batch["label"]

        assert (
            x.shape[1] == self.input_channels
        ), f"Expected {self.input_channels} input channels but got {x.shape[1]}"

        if not (0 <= x.min() and x.max() <= 1):
            print(
                f"Intensities of batch are not in (0, 1) but instead {(x.min(), x.max())}"
            )

        y_hat, mask = self._augment_and_forward(x)

        loss = self.rec_loss(y_hat, y, mask=mask if self.rec_loss_masked_only else None)

        if batch_idx == 0 and not self.disable_image_logging and not self.trainer.sanity_checking:
            self._log_debug_images(x, y, y_hat, stage="train", file_paths=batch["file_path"], idx=0)

        assert loss is not None, "Loss is None"
        assert torch.isfinite(loss).all(), f"Loss is not finite: {loss}"

        self.log_dict({"train/loss": loss})
        return loss

    def validation_step(self, batch, batch_idx):
        x, y = batch["image"], batch["label"]

        assert len(x.shape) == 5
        assert (
            x.shape[1] == self.input_channels
        ), f"Expected {self.input_channels} input channels but got {x.shape[1]}"

        if not (0 <= x.min() and x.max() <= 1):
            print(
                f"Intensities of batch are not in (0, 1) but instead {(x.min(), x.max())}"
            )

        y_hat, mask = self._augment_and_forward(x)
        loss = self.rec_loss(y_hat, y, mask=mask if self.rec_loss_masked_only else None)

        assert loss is not None, "Loss is None"
        assert torch.isfinite(loss).all(), f"Loss is not finite: {loss}"

        self.log_dict({"val/loss": loss})

    def rec_loss(self, y, y_hat, mask=None):
        """
        Reconstruction MSE loss. If a mask tensor is provided, the loss will only be calculated on masked tokens.
        """
        if mask is not None:
            y = y.clone()
            y_hat = y_hat.clone()
            y[~mask] = 0
            y_hat[~mask] = 0

        return self._rec_loss_fn(y, y_hat)

    def _augment_and_forward(self, x):
        with torch.no_grad():
            x, mask = random_mask(x, self.mask_ratio, self.mask_patch_size)

        y_hat = self.model(x)

        assert y_hat is not None
        assert y_hat.shape == x.shape, f"Got shape: {y_hat.shape}, expected: {x.shape}"

        return y_hat, mask

    def configure_optimizers(self):
        assert self.optimizer in ["Adam", "AdamW"]

        if self.optimizer == "AdamW":
            optimizer = AdamW(self.parameters(), lr=self.learning_rate)
        elif self.optimizer == "Adam":
            optimizer = Adam(self.parameters(), lr=self.learning_rate)

        print(f"Using optimizer {self.optimizer}")

        # cosine_half_period is from max to min
        cosine_half_period = (
            int(self.cosine_period_ratio * self.epochs) - self.warmup_epochs
        )
        cosine_scheduler = CosineAnnealingLR(
            optimizer, T_max=cosine_half_period * self.steps_per_epoch
        )

        if self.warmup_epochs > 0:
            warmup_scheduler = LinearLR(
                optimizer,
                start_factor=1.0 / 1000,
                total_iters=self.warmup_epochs * self.steps_per_epoch,
            )

            scheduler = SequentialLR(
                optimizer,
                schedulers=[warmup_scheduler, cosine_scheduler],
                milestones=[self.warmup_epochs * self.steps_per_epoch],
            )
        else:
            scheduler = cosine_scheduler

        scheduler_config = {
            "scheduler": scheduler,
            "interval": "step",
            "frequency": 1,  # scheduler is updated after each batch
        }

        return [optimizer], [scheduler_config]

    def load_state_dict(self, state_dict, *args, **kwargs):
        # First we filter out layers that have changed in size
        # This is often the case in the output layer.
        # If we are finetuning on a task with a different number of classes
        # than the pretraining task, the # output channels will have changed.
        old_params = copy.deepcopy(self.state_dict())
        state_dict = {
            k: v
            for k, v in state_dict.items()
            if (k in old_params) and (old_params[k].shape == state_dict[k].shape)
        }
        rejected_keys_new = [k for k in state_dict.keys() if k not in old_params]
        rejected_keys_shape = [
            k for k in state_dict.keys() if old_params[k].shape != state_dict[k].shape
        ]
        rejected_keys_data = []

        # Here there's also potential to implement custom loading functions.
        # E.g. to load 2D pretrained models into 3D by repeating or something like that.

        # Now keep track of the # of layers with succesful weight transfers
        successful = 0
        unsuccessful = 0
        super().load_state_dict(state_dict, *args, **kwargs)
        new_params = self.state_dict()
        for param_name, p1, p2 in zip(
            old_params.keys(), old_params.values(), new_params.values()
        ):
            # If more than one param in layer is NE (not equal) to the original weights we've successfully loaded new weights.
            if p1.data.ne(p2.data).sum() > 0:
                successful += 1
            else:
                unsuccessful += 1
                if (
                    param_name not in rejected_keys_new
                    and param_name not in rejected_keys_shape
                ):
                    rejected_keys_data.append(param_name)

        print(
            f"Succesfully transferred weights for {successful}/{successful+unsuccessful} layers"
        )
        print(
            f"Rejected the following keys:\n"
            f"Not in old dict: {rejected_keys_new}.\n"
            f"Wrong shape: {rejected_keys_shape}.\n"
            f"Post check not succesful: {rejected_keys_data}."
        )

    def _log_debug_images(self, x, y, y_hat, stage, file_paths, idx=0):
        examples = {}
        if self.current_epoch % 5 == 0:
            imgs = viz.get_imgs(x, y, y_hat, slice_dim=0, n=4, desc=file_paths[idx], idx=idx)
            examples[f"{stage}/examples/imgs"] = imgs

        if examples != {}:
            wandb_logger = self.loggers[0]

            assert isinstance(
                wandb_logger, WandbLogger
            ), f"Tried to log 3d image, but provided logger was not a Wandb logger, but instead {type(wandb_logger)}"
            wandb_logger.experiment.log(examples)


===============================================
File: models/supervised_base.py
===============================================

from typing import Optional
import torch
from torch.optim import AdamW
import copy
import logging

import lightning as L
from yucca.pipeline.preprocessing import YuccaPreprocessor
from yucca.functional.utils.kwargs import filter_kwargs
from batchgenerators.utilities.file_and_folder_operations import join
from models import networks


class BaseSupervisedModel(L.LightningModule):
    """
    Base class for supervised models (segmentation, classification, regression).
    Implements common functionality and defines abstract methods that subclasses must implement.
    """

    def __init__(
        self,
        config: dict = {},
        learning_rate: float = 1e-3,
        do_compile: Optional[bool] = False,
        compile_mode: Optional[str] = "default",
        weight_decay: float = 3e-5,
        amsgrad: bool = False,
        eps: float = 1e-8,
        betas: tuple = (0.9, 0.999),
        deep_supervision: bool = False,
    ):
        super().__init__()

        self.num_classes = config["num_classes"]
        self.num_modalities = config["num_modalities"]
        self.patch_size = config["patch_size"]
        self.plans = config.get("plans", {})
        self.model_name = config["model_name"]
        self.version_dir = config["version_dir"]
        self.task_type = config["task_type"]  # Added task_type property

        self.sliding_window_prediction = True
        self.sliding_window_overlap = 0.5  # nnUNet default
        self.test_time_augmentation = False
        self.progress_bar = True

        self.do_compile = do_compile
        self.compile_mode = compile_mode

        # Loss
        self.deep_supervision = deep_supervision

        # Optimizer
        self.learning_rate = learning_rate
        self.weight_decay = weight_decay
        self.amsgrad = amsgrad
        self.eps = eps
        self.betas = betas

        # Set up metrics in subclasses
        self.train_metrics = self._configure_metrics(prefix="train")
        self.val_metrics = self._configure_metrics(prefix="val")

        self.save_hyperparameters()
        self.load_model()

        self.model = (
            torch.compile(self.model, mode=self.compile_mode)
            if self.do_compile
            else self.model
        )

    def _configure_metrics(self, prefix: str):
        """
        Configure metrics specific to the task type.
        Must be implemented by subclasses.

        Args:
            prefix: Prefix for metric names (train or val)

        Returns:
            MetricCollection: Collection of metrics for the task
        """
        raise NotImplementedError("Subclasses must implement _configure_metrics")

    def _configure_losses(self):
        """
        Configure loss functions specific to the task type.
        Must be implemented by subclasses.

        Returns:
            tuple: (train_loss_fn, val_loss_fn)
        """
        raise NotImplementedError("Subclasses must implement _configure_losses")

    def load_model(self):
        """Load the appropriate model architecture"""
        print(f"Loading Model: 3D {self.model_name}")
        model_class = getattr(networks, self.model_name)

        print("Found model class: ", model_class)

        conv_op = torch.nn.Conv3d
        norm_op = torch.nn.InstanceNorm3d
        print("MODALITIES", self.num_modalities)

        # Pass task_type directly to UNet without mapping
        model_kwargs = {
            # Applies to all models
            "input_channels": self.num_modalities,
            "num_classes": self.num_classes,
            "output_channels": self.num_classes,
            "deep_supervision": self.deep_supervision,
            # Applies to most CNN-based architectures
            "conv_op": conv_op,
            # Applies to most CNN-based architectures (exceptions: UXNet)
            "norm_op": norm_op,
            # MedNeXt
            "checkpoint_style": None,
            # ensure not pretraining
            "mode": self.task_type,  # Pass task_type directly
        }
        model_kwargs = filter_kwargs(model_class, model_kwargs)
        self.model = model_class(**model_kwargs)

    def configure_optimizers(self):
        """Configure optimizers and learning rate schedulers"""
        # Set up task-specific loss functions
        self.loss_fn_train, self.loss_fn_val = self._configure_losses()

        self.optim = AdamW(
            self.model.parameters(),
            lr=self.learning_rate,
            weight_decay=self.weight_decay,
            amsgrad=self.amsgrad,
            eps=self.eps,
            betas=self.betas,
        )

        # Scheduler with early cut-off factor of 1.15
        self.lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(
            self.optim, T_max=int(self.trainer.max_epochs * 1.15), eta_min=1e-9
        )

        # Return the optimizer and scheduler - the loss is not returned
        return {"optimizer": self.optim, "lr_scheduler": self.lr_scheduler}

    def forward(self, inputs):
        """Forward pass through the model"""
        return self.model(inputs)

    def _process_batch(self, batch):
        """Process batch data - can be overridden by subclasses if needed"""
        inputs, target, file_path = batch["image"], batch["label"], batch["file_path"]
        return inputs, target, file_path

    def training_step(self, batch, _batch_idx):
        """Training step"""
        inputs, target, _ = self._process_batch(batch)

        output = self(inputs)
        loss = self.loss_fn_train(output, target)

        if self.deep_supervision and hasattr(output, "__iter__"):
            # If deep_supervision is enabled, output and target will be a list of (downsampled) tensors.
            # We only need the original ground truth and its corresponding prediction which is always the first entry in each list.
            output = output[0]
            target = target[0]

        metrics = self.compute_metrics(self.train_metrics, output, target)
        self.log_dict(
            {"train/loss": loss} | metrics,
            prog_bar=self.progress_bar,
            logger=True,
            sync_dist=True,
            on_step=True,
            on_epoch=True,
        )

        return loss

    def validation_step(self, batch, _batch_idx):
        """Validation step"""
        inputs, target, _ = self._process_batch(batch)

        output = self(inputs)
        loss = self.loss_fn_val(output, target)
        metrics = self.compute_metrics(self.val_metrics, output, target)
        self.log_dict(
            {"val/loss": loss} | metrics,
            prog_bar=self.progress_bar,
            logger=True,
            sync_dist=True,
            on_step=False,
            on_epoch=True,
        )

    def on_predict_start(self):
        """Set up for prediction"""
        self.preprocessor = YuccaPreprocessor(join(self.version_dir, "hparams.yaml"))

    def predict_step(self, batch, _batch_idx, _dataloader_idx=0):
        """Prediction step"""
        case, case_id = batch
        (
            case_preprocessed,
            case_properties,
        ) = self.preprocessor.preprocess_case_for_inference(
            case, self.patch_size, self.sliding_window_prediction
        )

        predictions = self.model.predict(
            data=case_preprocessed,
            mode="3D",
            mirror=self.test_time_augmentation,
            overlap=self.sliding_window_overlap,
            patch_size=self.patch_size,
            sliding_window_prediction=self.sliding_window_prediction,
            device=self.device,
        )
        predictions, case_properties = self.preprocessor.reverse_preprocessing(
            predictions, case_properties
        )
        return {
            "predictions": predictions,
            "properties": case_properties,
            "case_id": case_id[0],
        }

    def compute_metrics(self, metrics, output, target, ignore_index=None):
        """
        Compute task-specific metrics.
        Should be implemented/extended by subclasses for task-specific metrics.
        """
        raise NotImplementedError("Subclasses must implement compute_metrics")

    def load_state_dict(self, state_dict, *args, **kwargs):
        """Load state dict with handling for different model architectures"""
        # First we filter out layers that have changed in size
        # This is often the case in the output layer.
        # If we are finetuning on a task with a different number of classes
        # than the pretraining task, the # output channels will have changed.
        old_params = copy.deepcopy(self.state_dict())
        state_dict = {
            k: v
            for k, v in state_dict.items()
            if (k in old_params) and (old_params[k].shape == state_dict[k].shape)
        }
        rejected_keys_new = [k for k in state_dict.keys() if k not in old_params]
        rejected_keys_shape = [
            k for k in state_dict.keys() if old_params[k].shape != state_dict[k].shape
        ]
        rejected_keys_data = []

        # Here there's also potential to implement custom loading functions.
        # E.g. to load 2D pretrained models into 3D by repeating or something like that.

        # Now keep track of the # of layers with succesful weight transfers
        successful = 0
        unsuccessful = 0
        super().load_state_dict(state_dict, *args, **kwargs)
        new_params = self.state_dict()
        for param_name, p1, p2 in zip(
            old_params.keys(), old_params.values(), new_params.values()
        ):
            # If more than one param in layer is NE (not equal) to the original weights we've successfully loaded new weights.
            if p1.data.ne(p2.data).sum() > 0:
                successful += 1
            else:
                unsuccessful += 1
                if (
                    param_name not in rejected_keys_new
                    and param_name not in rejected_keys_shape
                ):
                    rejected_keys_data.append(param_name)

        logging.warn(
            f"Succesfully transferred weights for {successful}/{successful+unsuccessful} layers"
        )
        logging.warn(
            f"Rejected the following keys:\n"
            f"Not in old dict: {rejected_keys_new}.\n"
            f"Wrong shape: {rejected_keys_shape}.\n"
            f"Post check not succesful: {rejected_keys_data}."
        )

        return successful

    @staticmethod
    def create(task_type, config, **kwargs):
        """
        Factory method to create the appropriate model based on task type

        Args:
            task_type: Type of task (segmentation, classification, regression)
            config: Configuration dictionary
            **kwargs: Additional arguments for the model

        Returns:
            BaseSupervisedModel: Instance of appropriate model subclass
        """
        if task_type == "segmentation":
            from models.supervised_seg import SupervisedSegModel

            return SupervisedSegModel(config=config, **kwargs)
        elif task_type == "classification":
            from models.supervised_cls import SupervisedClsModel

            return SupervisedClsModel(config=config, **kwargs)
        elif task_type == "regression":
            from models.supervised_reg import SupervisedRegModel

            return SupervisedRegModel(config=config, **kwargs)
        else:
            raise ValueError(f"Unsupported task type: {task_type}")


===============================================
File: models/supervised_cls.py
===============================================

from typing import Optional
import torch
import torch.nn.functional as F
from torchmetrics import MetricCollection
from torchmetrics.classification import Accuracy, Precision, Recall, F1Score, AUROC

from models.supervised_base import BaseSupervisedModel


class SupervisedClsModel(BaseSupervisedModel):
    """
    Supervised model for classification tasks.
    Inherits from BaseSupervisedModel and implements classification-specific functionality.
    """

    def __init__(
        self,
        config: dict = {},
        learning_rate: float = 1e-3,
        do_compile: Optional[bool] = False,
        compile_mode: Optional[str] = "default",
        weight_decay: float = 3e-5,
        amsgrad: bool = False,
        eps: float = 1e-8,
        betas: tuple = (0.9, 0.999),
    ):
        super().__init__(
            config=config,
            learning_rate=learning_rate,
            do_compile=do_compile,
            compile_mode=compile_mode,
            weight_decay=weight_decay,
            amsgrad=amsgrad,
            eps=eps,
            betas=betas,
            deep_supervision=False,  # Classification doesn't use deep supervision
        )

    def _configure_metrics(self, prefix: str):
        """
        Configure classification-specific metrics

        Args:
            prefix: Prefix for metric names (train or val)

        Returns:
            MetricCollection: Collection of classification metrics
        """
        return MetricCollection(
            {
                f"{prefix}/accuracy": Accuracy(
                    task="multiclass", num_classes=self.num_classes
                ),
                f"{prefix}/precision": Precision(
                    task="multiclass", num_classes=self.num_classes, average="macro"
                ),
                f"{prefix}/recall": Recall(
                    task="multiclass", num_classes=self.num_classes, average="macro"
                ),
                f"{prefix}/f1": F1Score(
                    task="multiclass", num_classes=self.num_classes, average="macro"
                ),
            }
        )

    def _configure_losses(self):
        """
        Configure classification-specific loss functions

        Returns:
            tuple: (train_loss_fn, val_loss_fn)
        """
        # For classification, we typically use cross-entropy loss
        loss_fn = torch.nn.CrossEntropyLoss()
        return loss_fn, loss_fn

    def _process_batch(self, batch):
        """
        Process classification batch data

        Args:
            batch: Input batch

        Returns:
            tuple: (inputs, target, file_path)
        """
        inputs, target, file_path = batch["image"], batch["label"], batch["file_path"]
        # Convert target to long for classification tasks
        target = target.long()

        # Only squeeze if dimension exists
        if target.dim() > 1:
            target = target.squeeze(1)

        return inputs, target, file_path

    def compute_metrics(self, metrics, output, target, ignore_index=None):
        """
        Compute classification metrics

        Args:
            metrics: Metrics collection
            output: Model output
            target: Ground truth
            ignore_index: Index to ignore in metrics (not used in classification)

        Returns:
            dict: Dictionary of computed metrics
        """
        # Use the same approach for binary and multi-class classification
        # Apply softmax to get probabilities
        probabilities = F.softmax(output, dim=1)
        return metrics(probabilities, target)


===============================================
File: models/supervised_reg.py
===============================================

from typing import Optional
import torch
from torchmetrics import MetricCollection
from torchmetrics.regression import MeanSquaredError, MeanAbsoluteError, R2Score

from models.supervised_base import BaseSupervisedModel


class SupervisedRegModel(BaseSupervisedModel):
    """
    Supervised model for regression tasks.
    Inherits from BaseSupervisedModel and implements regression-specific functionality.
    """

    def __init__(
        self,
        config: dict = {},
        learning_rate: float = 1e-3,
        do_compile: Optional[bool] = False,
        compile_mode: Optional[str] = "default",
        weight_decay: float = 3e-5,
        amsgrad: bool = False,
        eps: float = 1e-8,
        betas: tuple = (0.9, 0.999),
    ):
        super().__init__(
            config=config,
            learning_rate=learning_rate,
            do_compile=do_compile,
            compile_mode=compile_mode,
            weight_decay=weight_decay,
            amsgrad=amsgrad,
            eps=eps,
            betas=betas,
            deep_supervision=False,  # Regression doesn't use deep supervision
        )

    def _configure_metrics(self, prefix: str):
        """
        Configure regression-specific metrics

        Args:
            prefix: Prefix for metric names (train or val)

        Returns:
            MetricCollection: Collection of regression metrics
        """
        return MetricCollection(
            {
                f"{prefix}/mse": MeanSquaredError(),
                f"{prefix}/mae": MeanAbsoluteError(),
                f"{prefix}/r2": R2Score(),
            }
        )

    def _configure_losses(self):
        """
        Configure regression-specific loss functions

        Returns:
            tuple: (train_loss_fn, val_loss_fn)
        """
        # For regression, we typically use MSE loss
        loss_fn = torch.nn.MSELoss()
        return loss_fn, loss_fn

    def _process_batch(self, batch):
        """
        Process regression batch data

        Args:
            batch: Input batch

        Returns:
            tuple: (inputs, target, file_path)
        """
        inputs, target, file_path = batch["image"], batch["label"], batch["file_path"]
        # Keep target as float for regression tasks
        target = target.float()
        return inputs, target, file_path

    def compute_metrics(self, metrics, output, target, ignore_index=None):
        """
        Compute regression metrics

        Args:
            metrics: Metrics collection
            output: Model output
            target: Ground truth
            ignore_index: Index to ignore in metrics (not used in regression)

        Returns:
            dict: Dictionary of computed metrics
        """
        return metrics(output, target)


===============================================
File: models/supervised_seg.py
===============================================

from typing import Optional
import torch
from torchmetrics import MetricCollection
from torchmetrics.classification import Dice

from yucca.modules.optimization.loss_functions.deep_supervision import (
    DeepSupervisionLoss,
)
from yucca.modules.optimization.loss_functions.nnUNet_losses import DiceCE
from yucca.modules.metrics.training_metrics import F1

from models.supervised_base import BaseSupervisedModel


class SupervisedSegModel(BaseSupervisedModel):
    """
    Supervised model for segmentation tasks.
    Inherits from BaseSupervisedModel and implements segmentation-specific functionality.
    """

    def __init__(
        self,
        config: dict = {},
        learning_rate: float = 1e-3,
        do_compile: Optional[bool] = False,
        compile_mode: Optional[str] = "default",
        weight_decay: float = 3e-5,
        amsgrad: bool = False,
        eps: float = 1e-8,
        betas: tuple = (0.9, 0.999),
        deep_supervision: bool = False,
    ):
        super().__init__(
            config=config,
            learning_rate=learning_rate,
            do_compile=do_compile,
            compile_mode=compile_mode,
            weight_decay=weight_decay,
            amsgrad=amsgrad,
            eps=eps,
            betas=betas,
            deep_supervision=deep_supervision,
        )

    def _configure_metrics(self, prefix: str):
        """
        Configure segmentation-specific metrics

        Args:
            prefix: Prefix for metric names (train or val)

        Returns:
            MetricCollection: Collection of segmentation metrics
        """
        return MetricCollection(
            {
                f"{prefix}/dice": Dice(
                    num_classes=self.num_classes,
                    ignore_index=0 if self.num_classes > 1 else None,
                ),
                f"{prefix}/F1": F1(
                    num_classes=self.num_classes,
                    ignore_index=0 if self.num_classes > 1 else None,
                    average=None,
                ),
            },
        )

    def _configure_losses(self):
        """
        Configure segmentation-specific loss functions

        Returns:
            tuple: (train_loss_fn, val_loss_fn)
        """
        loss_fn_train = DiceCE(soft_dice_kwargs={"apply_softmax": True})
        loss_fn_val = DiceCE(soft_dice_kwargs={"apply_softmax": True})

        if self.deep_supervision:
            loss_fn_train = DeepSupervisionLoss(loss_fn_train, weights=None)

        return loss_fn_train, loss_fn_val

    def compute_metrics(self, metrics, output, target, ignore_index: int = 0):
        """
        Compute segmentation metrics, handling per-class results

        Args:
            metrics: Metrics collection
            output: Model output
            target: Ground truth
            ignore_index: Index to ignore in metrics (usually background)

        Returns:
            dict: Dictionary of computed metrics
        """
        metrics = metrics(output, target)
        tmp = {}
        to_drop = []
        for key in metrics.keys():
            if metrics[key].numel() > 1:
                to_drop.append(key)
                for i, val in enumerate(metrics[key]):
                    if not i == ignore_index:
                        tmp[key + "_" + str(i)] = val
        for k in to_drop:
            metrics.pop(k)
        metrics.update(tmp)
        return metrics


===============================================
File: models/conv_blocks/__init__.py
===============================================




===============================================
File: models/conv_blocks/blocks.py
===============================================

import torch.nn as nn


class ConvDropoutNormNonlin(nn.Module):
    """
    2D Convolutional layers
    Arguments:
    num_in_filters {int} -- number of input filters
    num_out_filters {int} -- number of output filters
    kernel_size {tuple} -- size of the convolving kernel
    stride {tuple} -- stride of the convolution (default: {(1, 1)})
    activation {str} -- activation function (default: {'relu'})
    """

    def __init__(
        self,
        input_channels,
        output_channels,
        conv_op=nn.Conv2d,
        conv_kwargs={
            "kernel_size": 3,
            "stride": 1,
            "padding": 1,
            "dilation": 1,
            "bias": True,
        },
        norm_op=nn.BatchNorm2d,
        norm_op_kwargs={"eps": 1e-5, "affine": True, "momentum": 0.1},
        dropout_op=nn.Dropout2d,
        dropout_op_kwargs={"p": 0.5, "inplace": True},
        nonlin=nn.LeakyReLU,
        nonlin_kwargs={"negative_slope": 1e-2, "inplace": True},
    ):
        super().__init__()

        self.nonlin_kwargs = nonlin_kwargs
        self.nonlin = nonlin
        self.dropout_op = dropout_op
        self.dropout_op_kwargs = dropout_op_kwargs
        self.norm_op_kwargs = norm_op_kwargs
        self.conv_kwargs = conv_kwargs
        self.conv_op = conv_op
        self.norm_op = norm_op

        self.conv = self.conv_op(input_channels, output_channels, **self.conv_kwargs)

        if self.dropout_op is not None and self.dropout_op_kwargs["p"] is not None and self.dropout_op_kwargs["p"] > 0:
            self.dropout = self.dropout_op(**self.dropout_op_kwargs)
        else:
            self.dropout = None
        self.norm = self.norm_op(output_channels, **self.norm_op_kwargs)
        self.activation = self.nonlin(**self.nonlin_kwargs)

    def forward(self, x):
        x = self.conv(x)
        if self.dropout is not None:
            x = self.dropout(x)
        return self.activation(self.norm(x))


class ConvDropoutNorm(ConvDropoutNormNonlin):
    def forward(self, x):
        x = self.conv(x)
        if self.dropout is not None:
            x = self.dropout(x)
        return self.norm(x)


class DoubleConvDropoutNormNonlin(nn.Module):
    """
    2D Convolutional layers
    Arguments:
    num_in_filters {int} -- number of input filters
    num_out_filters {int} -- number of output filters
    kernel_size {tuple} -- size of the convolving kernel
    stride {tuple} -- stride of the convolution (default: {(1, 1)})
    activation {str} -- activation function (default: {'relu'})
    """

    def __init__(
        self,
        input_channels,
        output_channels,
        conv_op=nn.Conv2d,
        conv_kwargs={
            "kernel_size": 3,
            "stride": 1,
            "padding": 1,
            "dilation": 1,
            "bias": True,
        },
        norm_op=nn.BatchNorm2d,
        norm_op_kwargs={"eps": 1e-5, "affine": True, "momentum": 0.1},
        dropout_op=nn.Dropout2d,
        dropout_op_kwargs={"p": 0.5, "inplace": True},
        nonlin=nn.LeakyReLU,
        nonlin_kwargs={"negative_slope": 1e-2, "inplace": True},
    ):
        super().__init__()

        self.nonlin_kwargs = nonlin_kwargs
        self.nonlin = nonlin
        self.dropout_op = dropout_op
        self.dropout_op_kwargs = dropout_op_kwargs
        self.norm_op_kwargs = norm_op_kwargs
        self.conv_kwargs = conv_kwargs
        self.conv_op = conv_op
        self.norm_op = norm_op

        self.conv1 = ConvDropoutNormNonlin(
            input_channels,
            output_channels,
            self.conv_op,
            self.conv_kwargs,
            self.norm_op,
            self.norm_op_kwargs,
            self.dropout_op,
            self.dropout_op_kwargs,
            self.nonlin,
            self.nonlin_kwargs,
        )
        self.conv2 = ConvDropoutNormNonlin(
            output_channels,
            output_channels,
            self.conv_op,
            self.conv_kwargs,
            self.norm_op,
            self.norm_op_kwargs,
            self.dropout_op,
            self.dropout_op_kwargs,
            self.nonlin,
            self.nonlin_kwargs,
        )

    def forward(self, x):
        x = self.conv1(x)
        x = self.conv2(x)
        return x


class MultiLayerConvDropoutNormNonlin(nn.Module):
    """
    2D Convolutional layers
    Arguments:
    num_in_filters {int} -- number of input filters
    num_out_filters {int} -- number of output filters
    num_layers {int} -- number of conv layers, must be at least 1
    kernel_size {tuple} -- size of the convolving kernel
    stride {tuple} -- stride of the convolution (default: {(1, 1)})
    activation {str} -- activation function (default: {'relu'})
    """

    def __init__(
        self,
        input_channels,
        output_channels,
        num_layers=2,
        conv_op=nn.Conv2d,
        conv_kwargs={
            "kernel_size": 3,
            "stride": 1,
            "padding": 1,
            "dilation": 1,
            "bias": True,
        },
        norm_op=nn.BatchNorm2d,
        norm_op_kwargs={"eps": 1e-5, "affine": True, "momentum": 0.1},
        dropout_op=nn.Dropout2d,
        dropout_op_kwargs={"p": 0.5, "inplace": True},
        nonlin=nn.LeakyReLU,
        nonlin_kwargs={"negative_slope": 1e-2, "inplace": True},
    ):
        super().__init__()

        self.nonlin_kwargs = nonlin_kwargs
        self.nonlin = nonlin
        self.dropout_op = dropout_op
        self.dropout_op_kwargs = dropout_op_kwargs
        self.norm_op_kwargs = norm_op_kwargs
        self.conv_kwargs = conv_kwargs
        self.conv_op = conv_op
        self.norm_op = norm_op

        assert num_layers >= 1, "Number of layers must be at least 1, got {}".format(num_layers)
        self.num_layers = num_layers

        self.conv1 = ConvDropoutNormNonlin(
            input_channels,
            output_channels,
            self.conv_op,
            self.conv_kwargs,
            self.norm_op,
            self.norm_op_kwargs,
            self.dropout_op,
            self.dropout_op_kwargs,
            self.nonlin,
            self.nonlin_kwargs,
        )

        for layer in range(2, num_layers + 1):
            setattr(
                self,
                f"conv{layer}",
                ConvDropoutNormNonlin(
                    output_channels,
                    output_channels,
                    self.conv_op,
                    self.conv_kwargs,
                    self.norm_op,
                    self.norm_op_kwargs,
                    self.dropout_op,
                    self.dropout_op_kwargs,
                    self.nonlin,
                    self.nonlin_kwargs,
                ),
            )

    def forward(self, x):
        x = self.conv1(x)
        for layer in range(2, self.num_layers + 1):
            x = getattr(self, f"conv{layer}")(x)

        return x

    @staticmethod
    def get_block_constructor(n_layers):
        def _block(input_channels, output_channels, **kwargs):
            return MultiLayerConvDropoutNormNonlin(input_channels, output_channels, num_layers=n_layers, **kwargs)

        return _block


class DoubleLayerResBlock(nn.Module):
    """
    2D Convolutional layers
    Arguments:
    num_in_filters {int} -- number of input filters
    num_out_filters {int} -- number of output filters
    num_layers {int} -- number of conv layers, must be at least 1
    kernel_size {tuple} -- size of the convolving kernel
    stride {tuple} -- stride of the convolution (default: {(1, 1)})
    activation {str} -- activation function (default: {'relu'})
    """

    def __init__(
        self,
        input_channels,
        output_channels,
        conv_op=nn.Conv2d,
        conv_kwargs={
            "kernel_size": 3,
            "stride": 1,
            "padding": 1,
            "dilation": 1,
            "bias": True,
        },
        norm_op=nn.BatchNorm2d,
        norm_op_kwargs={"eps": 1e-5, "affine": True, "momentum": 0.1},
        dropout_op=nn.Dropout2d,
        dropout_op_kwargs={"p": 0.0, "inplace": True},
        nonlin=nn.LeakyReLU,
        nonlin_kwargs={"negative_slope": 1e-2, "inplace": True},
    ):
        super().__init__()

        self.nonlin_kwargs = nonlin_kwargs
        self.nonlin = nonlin
        self.dropout_op = dropout_op
        self.dropout_op_kwargs = dropout_op_kwargs
        self.norm_op_kwargs = norm_op_kwargs
        self.conv_kwargs = conv_kwargs
        self.conv_op = conv_op
        self.norm_op = norm_op

        assert conv_kwargs["dilation"] == 1, "Dilation must be 1 for residual blocks"

        self.conv1 = ConvDropoutNormNonlin(
            input_channels,
            output_channels,
            self.conv_op,
            self.conv_kwargs,
            self.norm_op,
            self.norm_op_kwargs,
            self.dropout_op,
            self.dropout_op_kwargs,
            self.nonlin,
            self.nonlin_kwargs,
        )

        if (conv_kwargs["stride"] != 1) or (input_channels != output_channels):
            self.downsample_skip = nn.Sequential(
                conv_op(input_channels, output_channels, kernel_size=1, padding=0, stride=conv_kwargs["stride"], bias=False),
                norm_op(output_channels, **norm_op_kwargs),
            )
        else:
            self.downsample_skip = lambda x: x

        self.conv2 = ConvDropoutNorm(
            output_channels,
            output_channels,
            self.conv_op,
            self.conv_kwargs,
            self.norm_op,
            self.norm_op_kwargs,
            self.dropout_op,
            self.dropout_op_kwargs,
            self.nonlin,
            self.nonlin_kwargs,
        )

        self.final_nonlin = self.nonlin(**self.nonlin_kwargs)

    def forward(self, x):
        residual = x

        x = self.conv1(x)
        x = self.conv2(x)

        x += self.downsample_skip(residual)
        x = self.final_nonlin(x)

        return x


class MultiLayerResBlock(nn.Module):
    """
    2D Convolutional layers
    Arguments:
    num_in_filters {int} -- number of input filters
    num_out_filters {int} -- number of output filters
    num_layers {int} -- number of conv layers, must be at least 1
    kernel_size {tuple} -- size of the convolving kernel
    stride {tuple} -- stride of the convolution (default: {(1, 1)})
    activation {str} -- activation function (default: {'relu'})
    """

    def __init__(
        self,
        input_channels,
        output_channels,
        num_layers=2,
        conv_op=nn.Conv2d,
        conv_kwargs={
            "kernel_size": 3,
            "stride": 1,
            "padding": 1,
            "dilation": 1,
            "bias": True,
        },
        norm_op=nn.BatchNorm2d,
        norm_op_kwargs={"eps": 1e-5, "affine": True, "momentum": 0.1},
        dropout_op=nn.Dropout2d,
        dropout_op_kwargs={"p": 0.5, "inplace": True},
        nonlin=nn.LeakyReLU,
        nonlin_kwargs={"negative_slope": 1e-2, "inplace": True},
    ):
        super().__init__()

        self.nonlin_kwargs = nonlin_kwargs
        self.nonlin = nonlin
        self.dropout_op = dropout_op
        self.dropout_op_kwargs = dropout_op_kwargs
        self.norm_op_kwargs = norm_op_kwargs
        self.conv_kwargs = conv_kwargs
        self.conv_op = conv_op
        self.norm_op = norm_op

        assert num_layers >= 1, "Number of layers must be at least 1, got {}".format(num_layers)

        assert conv_kwargs["stride"] == 1, "Stride must be 1 for residual blocks"
        assert conv_kwargs["conv_dilation"] == 1, "Dilation must be 1 for residual blocks"

        self.num_layers = num_layers

        self.conv1 = ConvDropoutNormNonlin(
            input_channels,
            output_channels,
            self.conv_op,
            self.conv_kwargs,
            self.norm_op,
            self.norm_op_kwargs,
            self.dropout_op,
            self.dropout_op_kwargs,
            self.nonlin,
            self.nonlin_kwargs,
        )

        if (conv_kwargs["stride"] != 1) or (input_channels != output_channels):
            self.downsample_skip = nn.Sequential(
                conv_op(input_channels, output_channels, kernel_size=1, padding=0, stride=conv_kwargs["stride"], bias=False),
                norm_op(output_channels, **norm_op_kwargs),
            )
        else:
            self.downsample_skip = lambda x: x

        for layer in range(2, num_layers + 1):
            if layer < num_layers:
                setattr(
                    self,
                    f"conv{layer}",
                    ConvDropoutNormNonlin(
                        output_channels,
                        output_channels,
                        self.conv_op,
                        self.conv_kwargs,
                        self.norm_op,
                        self.norm_op_kwargs,
                        self.dropout_op,
                        self.dropout_op_kwargs,
                        self.nonlin,
                        self.nonlin_kwargs,
                    ),
                )
            else:
                # Last layer does not have activation, is added after residual
                setattr(
                    self,
                    f"conv{layer}",
                    ConvDropoutNorm(
                        output_channels,
                        output_channels,
                        self.conv_op,
                        self.conv_kwargs,
                        self.norm_op,
                        self.norm_op_kwargs,
                        self.dropout_op,
                        self.dropout_op_kwargs,
                        self.nonlin,
                        self.nonlin_kwargs,
                    ),
                )

        self.final_nonlin = self.nonlin(**self.nonlin_kwargs)

    def forward(self, x):
        residual = x
        x = self.conv1(x)
        for layer in range(2, self.num_layers + 1):
            x = getattr(self, f"conv{layer}")(x)

        x += self.downsample_skip(residual)
        x = self.final_nonlin(x)

        return x

    @staticmethod
    def get_block_constructor(n_layers):
        def _block(input_channels, output_channels, **kwargs):
            return MultiLayerResBlock(input_channels, output_channels, num_layers=n_layers, **kwargs)

        return _block


===============================================
File: models/conv_blocks/layers.py
===============================================

import torch.nn as nn


class DepthwiseSeparableConv(nn.Module):
    def __init__(
        self,
        input_channels,
        output_channels,
        conv_op=nn.Conv2d,
        kernel_size=3,
        stride=1,
        padding=1,
        dilation=1,
        bias=True,
    ):
        super().__init__()
        self.depthconv = conv_op(
            input_channels,
            input_channels,
            kernel_size,
            groups=input_channels,
            stride=stride,
            padding=padding,
            dilation=dilation,
            bias=bias,
        )
        self.pointwiseconv = conv_op(input_channels, output_channels, kernel_size=1)


===============================================
File: models/networks/__init__.py
===============================================

from .mednext import (
    mednext_s3,
    mednext_s3_lw_dec,
    mednext_s3_std_dec,
    mednext_m3,
    mednext_m3_lw_dec,
    mednext_m3_std_dec,
    mednext_l3,
    mednext_l3_lw_dec,
    mednext_l3_std_dec,
)

from .unet import unet_xl, unet_xl_lw_dec, unet_b, unet_b_lw_dec


===============================================
File: models/networks/heads.py
===============================================

import torch
import torch.nn as nn


class ClsRegHead(nn.Module):
    def __init__(self, in_channels, num_classes):
        super().__init__()
        self.global_pool = nn.AdaptiveAvgPool3d((1, 1, 1))
        self.fc = nn.Linear(in_channels, num_classes)

    def forward(self, x):
        x = x[-1]  # only use bottleneck repr
        x = self.global_pool(x)
        x = torch.flatten(x, 1)
        x = self.fc(x)
        return x


===============================================
File: models/networks/mednext.py
===============================================

from typing import Optional, Union
import torch.nn as nn
from models.networks.unet import light_weight_decoder, standard_decoder
from yucca.modules.networks.blocks_and_layers.conv_blocks import (
    MedNeXtBlock,
    MedNeXtDownBlock,
    MedNeXtUpBlock,
    OutBlock,
)
from yucca.modules.networks.networks.YuccaNet import YuccaNet


class MedNeXt(YuccaNet):
    """
    From the paper: https://arxiv.org/pdf/2303.09975.pdf
    code source: https://github.com/MIC-DKFZ/MedNeXt/tree/main
    """

    def __init__(
        self,
        input_channels: int,
        output_channels: int = 1,
        contrastive: bool = False,
        rotation: bool = False,
        reconstruction: bool = False,
        prediction: bool = False,
        conv_op=nn.Conv3d,
        starting_filters: int = 32,
        enc_exp_r: Union[int, list] = 2,
        dec_exp_r: Union[int, list] = 2,
        kernel_size: int = 5,
        do_res: bool = True,
        do_res_up_down: bool = True,
        enc_block_counts: list = [2, 2, 2, 2, 2],
        dec_block_counts: list = [2, 2, 2, 2],
        norm_type="group",
        grn=False,
    ):
        super().__init__()
        self.contrastive = contrastive
        self.rotation = rotation
        self.reconstruction = reconstruction
        self.prediction = prediction
        print(
            f"Loaded MedNeXt with Contrastive: {self.contrastive}, Rotation: {self.rotation}, Reconstruction: {self.reconstruction}, Prediction: {self.prediction}"
        )

        dim = starting_filters * 16

        self.num_classes = output_channels

        self.encoder = MedNeXtEncoder(
            input_channels=input_channels,
            conv_op=conv_op,
            starting_filters=starting_filters,
            kernel_size=kernel_size,
            exp_r=enc_exp_r,
            do_res=do_res,
            do_res_up_down=do_res_up_down,
            block_counts=enc_block_counts,
            norm_type=norm_type,
            grn=grn,
        )
        if self.contrastive:
            self.con_head = nn.Sequential(nn.AdaptiveAvgPool3d(1), nn.Flatten(), nn.Linear(dim, 512))

        if self.rotation:
            self.rot_head = nn.Sequential(nn.AdaptiveAvgPool3d(1), nn.Flatten(), nn.Linear(dim, 4), nn.Softmax(dim=1))

        # We dont use the mednext decoder during pretraining. Instantiate it here if you need it.
        self.rec_head = None
        self.pred_head = MedNeXtDecoder(
            output_channels=output_channels,
            starting_filters=starting_filters,
            exp_r=dec_exp_r,
            kernel_size=kernel_size,
            do_res=do_res,
            do_res_up_down=do_res_up_down,
            block_counts=dec_block_counts,
            norm_type=norm_type,
            grn=grn,
        )

    def forward(self, x):
        assert self.rec_head is not None or self.pred_head is not None

        enc = self.encoder(x)
        if self.prediction:
            return self.pred_head(enc)

        y_hat_rot = self.rot_head(enc[4]) if self.rotation else None
        y_hat_con = self.con_head(enc[4]) if self.contrastive else None
        y_hat_rec = self.rec_head(enc) if self.reconstruction else None

        return y_hat_con, y_hat_rec, y_hat_rot


class MedNeXtEncoder(nn.Module):
    def __init__(
        self,
        input_channels: int,
        conv_op=nn.Conv3d,
        starting_filters: int = 32,
        exp_r: Union[int, list] = [3, 4, 8, 8, 8],
        kernel_size: int = 5,
        do_res: bool = True,
        do_res_up_down: bool = True,
        block_counts: list = [3, 4, 8, 8, 8],
        norm_type="group",
        grn=False,
    ):
        super().__init__()

        dim = "3d"

        self.stem = conv_op(input_channels, starting_filters, kernel_size=1)
        if isinstance(exp_r, int):
            exp_r = [exp_r for i in range(len(block_counts))]

        self.enc_block_0 = nn.Sequential(
            *[
                MedNeXtBlock(
                    in_channels=starting_filters,
                    out_channels=starting_filters,
                    exp_r=exp_r[0],
                    kernel_size=kernel_size,
                    do_res=do_res,
                    norm_type=norm_type,
                    dim=dim,
                    grn=grn,
                )
                for i in range(block_counts[0])
            ]
        )

        self.down_0 = MedNeXtDownBlock(
            in_channels=starting_filters,
            out_channels=2 * starting_filters,
            exp_r=exp_r[1],
            kernel_size=kernel_size,
            do_res=do_res_up_down,
            norm_type=norm_type,
            dim=dim,
        )

        self.enc_block_1 = nn.Sequential(
            *[
                MedNeXtBlock(
                    in_channels=starting_filters * 2,
                    out_channels=starting_filters * 2,
                    exp_r=exp_r[1],
                    kernel_size=kernel_size,
                    do_res=do_res,
                    norm_type=norm_type,
                    dim=dim,
                    grn=grn,
                )
                for i in range(block_counts[1])
            ]
        )

        self.down_1 = MedNeXtDownBlock(
            in_channels=2 * starting_filters,
            out_channels=4 * starting_filters,
            exp_r=exp_r[2],
            kernel_size=kernel_size,
            do_res=do_res_up_down,
            norm_type=norm_type,
            dim=dim,
        )

        self.enc_block_2 = nn.Sequential(
            *[
                MedNeXtBlock(
                    in_channels=starting_filters * 4,
                    out_channels=starting_filters * 4,
                    exp_r=exp_r[2],
                    kernel_size=kernel_size,
                    do_res=do_res,
                    norm_type=norm_type,
                    dim=dim,
                    grn=grn,
                )
                for i in range(block_counts[2])
            ]
        )

        self.down_2 = MedNeXtDownBlock(
            in_channels=4 * starting_filters,
            out_channels=8 * starting_filters,
            exp_r=exp_r[3],
            kernel_size=kernel_size,
            do_res=do_res_up_down,
            norm_type=norm_type,
            dim=dim,
        )

        self.enc_block_3 = nn.Sequential(
            *[
                MedNeXtBlock(
                    in_channels=starting_filters * 8,
                    out_channels=starting_filters * 8,
                    exp_r=exp_r[3],
                    kernel_size=kernel_size,
                    do_res=do_res,
                    norm_type=norm_type,
                    dim=dim,
                    grn=grn,
                )
                for i in range(block_counts[3])
            ]
        )

        self.down_3 = MedNeXtDownBlock(
            in_channels=8 * starting_filters,
            out_channels=16 * starting_filters,
            exp_r=exp_r[4],
            kernel_size=kernel_size,
            do_res=do_res_up_down,
            norm_type=norm_type,
            dim=dim,
        )

        self.bottleneck = nn.Sequential(
            *[
                MedNeXtBlock(
                    in_channels=starting_filters * 16,
                    out_channels=starting_filters * 16,
                    exp_r=exp_r[4],
                    kernel_size=kernel_size,
                    do_res=do_res,
                    norm_type=norm_type,
                    dim=dim,
                    grn=grn,
                )
                for i in range(block_counts[4])
            ]
        )

    def forward(self, x):
        x = self.stem(x)
        x_res_0 = self.enc_block_0(x)
        x = self.down_0(x_res_0)
        x_res_1 = self.enc_block_1(x)
        x = self.down_1(x_res_1)
        x_res_2 = self.enc_block_2(x)
        x = self.down_2(x_res_2)
        x_res_3 = self.enc_block_3(x)
        x = self.down_3(x_res_3)

        x = self.bottleneck(x)
        return [x_res_0, x_res_1, x_res_2, x_res_3, x]


class MedNeXtDecoder(nn.Module):
    def __init__(
        self,
        output_channels: int = 1,
        starting_filters: int = 32,
        exp_r: Union[int, list] = [3, 4, 8, 8, 8, 8, 8, 4, 3],
        kernel_size: int = 5,
        dec_kernel_size: Optional[int] = None,
        deep_supervision: bool = False,
        do_res: bool = True,
        do_res_up_down: bool = True,
        block_counts: list = [8, 8, 4, 3],
        norm_type="group",
        grn=False,
    ):
        super().__init__()

        self.deep_supervision = deep_supervision
        self.output_channels = output_channels
        if kernel_size is not None:
            dec_kernel_size = kernel_size

        dim = "3d"

        if isinstance(exp_r, int):
            exp_r = [exp_r for i in range(len(block_counts))]

        self.up_3 = MedNeXtUpBlock(
            in_channels=16 * starting_filters,
            out_channels=8 * starting_filters,
            exp_r=exp_r[0],
            kernel_size=dec_kernel_size,
            do_res=do_res_up_down,
            norm_type=norm_type,
            dim=dim,
            grn=grn,
        )

        self.dec_block_3 = nn.Sequential(
            *[
                MedNeXtBlock(
                    in_channels=starting_filters * 8,
                    out_channels=starting_filters * 8,
                    exp_r=exp_r[0],
                    kernel_size=dec_kernel_size,
                    do_res=do_res,
                    norm_type=norm_type,
                    dim=dim,
                    grn=grn,
                )
                for i in range(block_counts[0])
            ]
        )

        self.up_2 = MedNeXtUpBlock(
            in_channels=8 * starting_filters,
            out_channels=4 * starting_filters,
            exp_r=exp_r[1],
            kernel_size=dec_kernel_size,
            do_res=do_res_up_down,
            norm_type=norm_type,
            dim=dim,
            grn=grn,
        )

        self.dec_block_2 = nn.Sequential(
            *[
                MedNeXtBlock(
                    in_channels=starting_filters * 4,
                    out_channels=starting_filters * 4,
                    exp_r=exp_r[1],
                    kernel_size=dec_kernel_size,
                    do_res=do_res,
                    norm_type=norm_type,
                    dim=dim,
                    grn=grn,
                )
                for i in range(block_counts[1])
            ]
        )

        self.up_1 = MedNeXtUpBlock(
            in_channels=4 * starting_filters,
            out_channels=2 * starting_filters,
            exp_r=exp_r[2],
            kernel_size=dec_kernel_size,
            do_res=do_res_up_down,
            norm_type=norm_type,
            dim=dim,
            grn=grn,
        )

        self.dec_block_1 = nn.Sequential(
            *[
                MedNeXtBlock(
                    in_channels=starting_filters * 2,
                    out_channels=starting_filters * 2,
                    exp_r=exp_r[2],
                    kernel_size=dec_kernel_size,
                    do_res=do_res,
                    norm_type=norm_type,
                    dim=dim,
                    grn=grn,
                )
                for i in range(block_counts[2])
            ]
        )

        self.up_0 = MedNeXtUpBlock(
            in_channels=2 * starting_filters,
            out_channels=starting_filters,
            exp_r=exp_r[3],
            kernel_size=dec_kernel_size,
            do_res=do_res_up_down,
            norm_type=norm_type,
            dim=dim,
            grn=grn,
        )

        self.dec_block_0 = nn.Sequential(
            *[
                MedNeXtBlock(
                    in_channels=starting_filters,
                    out_channels=starting_filters,
                    exp_r=exp_r[3],
                    kernel_size=dec_kernel_size,
                    do_res=do_res,
                    norm_type=norm_type,
                    dim=dim,
                    grn=grn,
                )
                for i in range(block_counts[3])
            ]
        )

        self.out_0 = OutBlock(in_channels=starting_filters, n_classes=self.output_channels, dim=dim)

        if self.deep_supervision:
            raise NotImplementedError

        self.block_counts = block_counts

    def forward(self, xs: list):
        # unpack the output of the encoder
        x_res_0, x_res_1, x_res_2, x_res_3, x = xs

        x_up_3 = self.up_3(x)
        dec_x = x_res_3 + x_up_3
        x = self.dec_block_3(dec_x)

        del x_res_3, x_up_3

        x_up_2 = self.up_2(x)
        dec_x = x_res_2 + x_up_2
        x = self.dec_block_2(dec_x)
        del x_res_2, x_up_2

        x_up_1 = self.up_1(x)
        dec_x = x_res_1 + x_up_1
        x = self.dec_block_1(dec_x)

        del x_res_1, x_up_1

        x_up_0 = self.up_0(x)
        dec_x = x_res_0 + x_up_0
        x = self.dec_block_0(dec_x)
        del x_res_0, x_up_0, dec_x

        x = self.out_0(x)

        return x


class MedNeXtDecoderSSL(nn.Module):
    def __init__(
        self,
        output_channels: int = 1,
        starting_filters: int = 32,
        exp_r=[3, 4, 8, 8, 8, 8, 8, 4, 3],  # Expansion ratio as in Swin Transformers
        kernel_size: int = 5,  # Ofcourse can test kernel_size
        dec_kernel_size: Optional[int] = None,
        deep_supervision: bool = False,  # Can be used to test deep supervision
        do_res: bool = True,  # Can be used to individually test residual connection
        do_res_up_down: bool = True,  # Additional 'res' connection on up and down convs
        block_counts: list = [3, 4, 8, 8, 8, 8, 8, 4, 3],  # Can be used to test staging ratio:
        norm_type="group",
        grn=False,
    ):
        super().__init__()

        self.deep_supervision = deep_supervision
        self.output_channels = output_channels

        if kernel_size is not None:
            dec_kernel_size = kernel_size

        dim = "3d"

        if isinstance(exp_r, int):
            exp_r = [exp_r for i in range(len(block_counts))]

        self.up_3 = MedNeXtUpBlock(
            in_channels=16 * starting_filters,
            out_channels=8 * starting_filters,
            exp_r=exp_r[5],
            kernel_size=dec_kernel_size,
            do_res=do_res_up_down,
            norm_type=norm_type,
            dim=dim,
            grn=grn,
        )

        self.dec_block_3 = nn.Sequential(
            *[
                MedNeXtBlock(
                    in_channels=starting_filters * 8,
                    out_channels=starting_filters * 8,
                    exp_r=exp_r[5],
                    kernel_size=dec_kernel_size,
                    do_res=do_res,
                    norm_type=norm_type,
                    dim=dim,
                    grn=grn,
                )
                for i in range(block_counts[5])
            ]
        )

        self.up_2 = MedNeXtUpBlock(
            in_channels=8 * starting_filters,
            out_channels=4 * starting_filters,
            exp_r=exp_r[6],
            kernel_size=dec_kernel_size,
            do_res=do_res_up_down,
            norm_type=norm_type,
            dim=dim,
            grn=grn,
        )

        self.dec_block_2 = nn.Sequential(
            *[
                MedNeXtBlock(
                    in_channels=starting_filters * 4,
                    out_channels=starting_filters * 4,
                    exp_r=exp_r[6],
                    kernel_size=dec_kernel_size,
                    do_res=do_res,
                    norm_type=norm_type,
                    dim=dim,
                    grn=grn,
                )
                for i in range(block_counts[6])
            ]
        )

        self.up_1 = MedNeXtUpBlock(
            in_channels=4 * starting_filters,
            out_channels=2 * starting_filters,
            exp_r=exp_r[7],
            kernel_size=dec_kernel_size,
            do_res=do_res_up_down,
            norm_type=norm_type,
            dim=dim,
            grn=grn,
        )

        self.dec_block_1 = nn.Sequential(
            *[
                MedNeXtBlock(
                    in_channels=starting_filters * 2,
                    out_channels=starting_filters * 2,
                    exp_r=exp_r[7],
                    kernel_size=dec_kernel_size,
                    do_res=do_res,
                    norm_type=norm_type,
                    dim=dim,
                    grn=grn,
                )
                for i in range(block_counts[7])
            ]
        )

        self.up_0 = MedNeXtUpBlock(
            in_channels=2 * starting_filters,
            out_channels=starting_filters,
            exp_r=exp_r[8],
            kernel_size=dec_kernel_size,
            do_res=do_res_up_down,
            norm_type=norm_type,
            dim=dim,
            grn=grn,
        )

        self.dec_block_0 = nn.Sequential(
            *[
                MedNeXtBlock(
                    in_channels=starting_filters,
                    out_channels=starting_filters,
                    exp_r=exp_r[8],
                    kernel_size=dec_kernel_size,
                    do_res=do_res,
                    norm_type=norm_type,
                    dim=dim,
                    grn=grn,
                )
                for i in range(block_counts[8])
            ]
        )

        self.out_0 = OutBlock(in_channels=starting_filters, n_classes=self.output_channels, dim=dim)

        if self.deep_supervision:
            raise NotImplementedError

        self.block_counts = block_counts

    def forward(self, x: list):
        x = self.up_3(x)
        x = self.dec_block_3(x)

        x = self.up_2(x)
        x = self.dec_block_2(x)

        x = self.up_1(x)
        x = self.dec_block_1(x)

        x = self.up_0(x)
        x = self.dec_block_0(x)

        x = self.out_0(x)

        return x


def mednext_s3(
    input_channels: int,
    num_classes: int = 1,
    conv_op=nn.Conv3d,
    contrastive: bool = False,
    rotation: bool = False,
    reconstruction: bool = False,
    prediction: bool = False,
    grn: bool = False,
    deep_supervision: bool = False,
):
    return MedNeXt(
        input_channels=input_channels,
        output_channels=num_classes,
        conv_op=conv_op,
        contrastive=contrastive,
        rotation=rotation,
        reconstruction=reconstruction,
        prediction=prediction,
        kernel_size=3,
        enc_exp_r=2,
        dec_exp_r=2,
        enc_block_counts=[2, 2, 2, 2, 2],
        dec_block_counts=[2, 2, 2, 2],
        grn=grn,
        deep_supervision=deep_supervision,
    )


def mednext_s3_lw_dec(
    input_channels: int,
    num_classes: int = 1,
    contrastive: bool = False,
    rotation: bool = False,
    reconstruction: bool = False,
):
    net = mednext_s3(
        input_channels=input_channels,
        num_classes=num_classes,
        contrastive=contrastive,
        rotation=rotation,
        reconstruction=reconstruction,
        prediction=False,
    )

    net.rec_head = light_weight_decoder(output_channels=num_classes, starting_filters=32, use_skip_connections=False)
    net.pred_head = None

    return net


def mednext_s3_std_dec(
    input_channels: int,
    num_classes: int = 1,
    contrastive: bool = False,
    rotation: bool = False,
    reconstruction: bool = False,
    prediction: bool = True,
    deep_supervision: bool = True,
):
    net = mednext_s3(
        input_channels=input_channels,
        num_classes=num_classes,
        contrastive=contrastive,
        rotation=rotation,
        reconstruction=reconstruction,
        prediction=prediction,
        deep_supervision=deep_supervision,
    )

    # sanity check
    assert (prediction and not reconstruction) or (reconstruction and not prediction)

    if reconstruction:
        assert not deep_supervision
        print("Using a standard unet decoder as reconstruction head")
        net.rec_head = standard_decoder(output_channels=num_classes, starting_filters=32, use_skip_connections=False)

    if prediction:
        print("Using a standard unet decoder as prediction head")
        net.pred_head = standard_decoder(
            output_channels=num_classes, starting_filters=32, use_skip_connections=True, deep_supervision=deep_supervision
        )

    return net


def mednext_m3(
    input_channels: int,
    num_classes: int = 1,
    conv_op=nn.Conv3d,
    contrastive: bool = False,
    rotation: bool = False,
    reconstruction: bool = False,
    prediction: bool = False,
):
    return MedNeXt(
        input_channels=input_channels,
        output_channels=num_classes,
        conv_op=conv_op,
        contrastive=contrastive,
        rotation=rotation,
        reconstruction=reconstruction,
        prediction=prediction,
        kernel_size=3,
        enc_exp_r=[2, 3, 4, 4, 4],
        dec_exp_r=[4, 4, 3, 2],
        enc_block_counts=[3, 4, 4, 4, 4],
        dec_block_counts=[4, 4, 4, 3],
    )


def mednext_m3_lw_dec(
    input_channels: int,
    num_classes: int = 1,
    contrastive: bool = False,
    rotation: bool = False,
    reconstruction: bool = False,
):
    net = mednext_m3(
        input_channels=input_channels,
        num_classes=num_classes,
        contrastive=contrastive,
        rotation=rotation,
        reconstruction=reconstruction,
        prediction=False,
    )

    net.rec_head = light_weight_decoder(output_channels=num_classes, starting_filters=32, use_skip_connections=False)
    net.pred_head = None

    return net


def mednext_m3_std_dec(
    input_channels: int,
    num_classes: int = 1,
    contrastive: bool = False,
    rotation: bool = False,
    reconstruction: bool = False,
    prediction: bool = True,
):
    net = mednext_m3(
        input_channels=input_channels,
        num_classes=num_classes,
        contrastive=contrastive,
        rotation=rotation,
        reconstruction=reconstruction,
        prediction=prediction,
    )

    # sanity check
    assert (prediction and not reconstruction) or (reconstruction and not prediction)

    if reconstruction:
        print("Using a standard unet decoder as reconstruction head")
        net.rec_head = standard_decoder(output_channels=num_classes, starting_filters=32, use_skip_connections=False)

    if prediction:
        print("Using a standard unet decoder as prediction head")
        net.pred_head = standard_decoder(output_channels=num_classes, starting_filters=32, use_skip_connections=True)

    return net


def mednext_l3(
    input_channels: int,
    num_classes: int = 1,
    conv_op=nn.Conv3d,
    contrastive: bool = False,
    rotation: bool = False,
    reconstruction: bool = False,
    prediction: bool = False,
):
    return MedNeXt(
        input_channels=input_channels,
        output_channels=num_classes,
        conv_op=conv_op,
        contrastive=contrastive,
        rotation=rotation,
        reconstruction=reconstruction,
        prediction=prediction,
        kernel_size=3,
        enc_exp_r=[3, 4, 8, 8, 8],
        dec_exp_r=[8, 8, 4, 3],
        enc_block_counts=[3, 4, 8, 8, 8],
        dec_block_counts=[8, 8, 4, 3],
    )


def mednext_l3_lw_dec(
    input_channels: int,
    num_classes: int = 1,
    contrastive: bool = False,
    rotation: bool = False,
    reconstruction: bool = False,
):
    net = mednext_l3(
        input_channels=input_channels,
        num_classes=num_classes,
        contrastive=contrastive,
        rotation=rotation,
        reconstruction=reconstruction,
        prediction=False,
    )

    net.rec_head = light_weight_decoder(output_channels=num_classes, starting_filters=32, use_skip_connections=False)
    net.pred_head = None

    return net


def mednext_l3_std_dec(
    input_channels: int,
    num_classes: int = 1,
    contrastive: bool = False,
    rotation: bool = False,
    reconstruction: bool = False,
    prediction: bool = True,
):
    net = mednext_l3(
        input_channels=input_channels,
        num_classes=num_classes,
        contrastive=contrastive,
        rotation=rotation,
        reconstruction=reconstruction,
        prediction=prediction,
    )

    # sanity check
    assert (prediction and not reconstruction) or (reconstruction and not prediction)

    if reconstruction:
        print("Using a standard unet decoder as reconstruction head")
        net.rec_head = standard_decoder(output_channels=num_classes, starting_filters=32, use_skip_connections=False)

    if prediction:
        print("Using a standard unet decoder as prediction head")
        net.pred_head = standard_decoder(output_channels=num_classes, starting_filters=32, use_skip_connections=True)

    return net


===============================================
File: models/networks/unet.py
===============================================

from typing import Literal

import torch
import torch.nn as nn

from yucca.modules.networks.networks.YuccaNet import YuccaNet
from models.conv_blocks.blocks import (
    DoubleConvDropoutNormNonlin,
    MultiLayerConvDropoutNormNonlin,
)
from models.networks.heads import ClsRegHead


class UNet(YuccaNet):
    def __init__(
        self,
        mode: Literal[
            "mae", "classification", "segmentation", "regression", "enc"
        ] = "segmentation",  # prediction mode
        input_channels: int = 1,
        output_channels: int = 1,
        starting_filters: int = 64,
        encoder_block: nn.Module = MultiLayerConvDropoutNormNonlin.get_block_constructor(
            2
        ),
        decoder_block: nn.Module = MultiLayerConvDropoutNormNonlin.get_block_constructor(
            2
        ),
        use_skip_connections: bool = False,
        deep_supervision: bool = False,
    ):
        super().__init__()

        self.encoder_block = encoder_block
        self.decoder_block = decoder_block

        self.encoder = UNetEncoder(
            input_channels=input_channels,
            starting_filters=starting_filters,
            basic_block=encoder_block,
        )
        self.num_classes = output_channels
        self.mode = mode

        # Set up the decoder based on the mode
        if mode == "mae":
            self.decoder = UNetDecoder(
                output_channels=output_channels,
                use_skip_connections=use_skip_connections,
                basic_block=decoder_block,
                starting_filters=starting_filters,
            )
        elif mode == "segmentation":
            self.decoder = UNetDecoder(
                output_channels=output_channels,
                use_skip_connections=True,
                deep_supervision=deep_supervision,
                basic_block=decoder_block,
                starting_filters=starting_filters,
            )
        elif mode in ["classification", "regression"]:
            self.decoder = ClsRegHead(
                in_channels=starting_filters * 16, num_classes=output_channels
            )
        elif mode == "enc":
            self.decoder = nn.Identity()
        else:
            raise ValueError(
                "Invalid mode. Choose from 'mae', 'segmentation', 'classification', 'regression', or 'enc'"
            )

    def forward(self, x):
        enc = self.encoder(x)
        return self.decoder(enc)


class UNetEncoder(nn.Module):
    def __init__(
        self,
        input_channels: int,
        starting_filters: int = 64,
        conv_op=nn.Conv3d,
        conv_kwargs={
            "kernel_size": 3,
            "stride": 1,
            "padding": 1,
            "dilation": 1,
            "bias": True,
        },
        norm_op=nn.InstanceNorm3d,
        norm_op_kwargs={"eps": 1e-5, "affine": True, "momentum": 0.1},
        dropout_op=nn.Dropout3d,
        dropout_op_kwargs={"p": 0.0, "inplace": True},
        nonlin=nn.LeakyReLU,
        nonlin_kwargs={"negative_slope": 1e-2, "inplace": True},
        weightInitializer=None,
        basic_block=DoubleConvDropoutNormNonlin,
    ) -> None:
        super().__init__()

        # Task specific
        self.filters = starting_filters

        # Model parameters
        self.conv_op = conv_op
        self.conv_kwargs = conv_kwargs
        self.norm_op_kwargs = norm_op_kwargs
        self.norm_op = norm_op
        self.dropout_op = dropout_op
        self.dropout_op_kwargs = dropout_op_kwargs
        self.nonlin_kwargs = nonlin_kwargs
        self.nonlin = nonlin
        self.weightInitializer = weightInitializer
        self.basic_block = basic_block

        self.pool_op = nn.MaxPool3d

        self.in_conv = self.basic_block(
            input_channels=input_channels,
            output_channels=self.filters,
            conv_op=self.conv_op,
            conv_kwargs=self.conv_kwargs,
            norm_op=self.norm_op,
            norm_op_kwargs=self.norm_op_kwargs,
            dropout_op=self.dropout_op,
            dropout_op_kwargs=self.dropout_op_kwargs,
            nonlin=self.nonlin,
            nonlin_kwargs=self.nonlin_kwargs,
        )

        self.pool1 = self.pool_op(2)
        self.encoder_conv1 = self.basic_block(
            input_channels=self.filters,
            output_channels=self.filters * 2,
            conv_op=self.conv_op,
            conv_kwargs=self.conv_kwargs,
            norm_op=self.norm_op,
            norm_op_kwargs=self.norm_op_kwargs,
            dropout_op=self.dropout_op,
            dropout_op_kwargs=self.dropout_op_kwargs,
            nonlin=self.nonlin,
            nonlin_kwargs=self.nonlin_kwargs,
        )

        self.pool2 = self.pool_op(2)
        self.encoder_conv2 = self.basic_block(
            input_channels=self.filters * 2,
            output_channels=self.filters * 4,
            conv_op=self.conv_op,
            conv_kwargs=self.conv_kwargs,
            norm_op=self.norm_op,
            norm_op_kwargs=self.norm_op_kwargs,
            dropout_op=self.dropout_op,
            dropout_op_kwargs=self.dropout_op_kwargs,
            nonlin=self.nonlin,
            nonlin_kwargs=self.nonlin_kwargs,
        )

        self.pool3 = self.pool_op(2)
        self.encoder_conv3 = self.basic_block(
            input_channels=self.filters * 4,
            output_channels=self.filters * 8,
            conv_op=self.conv_op,
            conv_kwargs=self.conv_kwargs,
            norm_op=self.norm_op,
            norm_op_kwargs=self.norm_op_kwargs,
            dropout_op=self.dropout_op,
            dropout_op_kwargs=self.dropout_op_kwargs,
            nonlin=self.nonlin,
            nonlin_kwargs=self.nonlin_kwargs,
        )

        self.pool4 = self.pool_op(2)
        self.encoder_conv4 = self.basic_block(
            input_channels=self.filters * 8,
            output_channels=self.filters * 16,
            conv_op=self.conv_op,
            conv_kwargs=self.conv_kwargs,
            norm_op=self.norm_op,
            norm_op_kwargs=self.norm_op_kwargs,
            dropout_op=self.dropout_op,
            dropout_op_kwargs=self.dropout_op_kwargs,
            nonlin=self.nonlin,
            nonlin_kwargs=self.nonlin_kwargs,
        )

        if self.weightInitializer is not None:
            print("initializing weights")
            self.apply(self.weightInitializer)

    def forward(self, x):
        x0 = self.in_conv(x)

        x1 = self.pool1(x0)
        x1 = self.encoder_conv1(x1)

        x2 = self.pool2(x1)
        x2 = self.encoder_conv2(x2)

        x3 = self.pool3(x2)
        x3 = self.encoder_conv3(x3)

        x4 = self.pool4(x3)
        x4 = self.encoder_conv4(x4)

        return [x0, x1, x2, x3, x4]


class UNetDecoder(nn.Module):
    def __init__(
        self,
        output_channels: int = 1,
        starting_filters: int = 64,
        conv_op=nn.Conv3d,
        conv_kwargs={
            "kernel_size": 3,
            "stride": 1,
            "padding": 1,
            "dilation": 1,
            "bias": True,
        },
        norm_op=nn.InstanceNorm3d,
        norm_op_kwargs={"eps": 1e-5, "affine": True, "momentum": 0.1},
        dropout_op=nn.Dropout3d,
        dropout_op_kwargs={"p": 0.0, "inplace": True},
        nonlin=nn.LeakyReLU,
        nonlin_kwargs={"negative_slope": 1e-2, "inplace": True},
        dropout_in_decoder=False,
        weightInitializer=None,
        basic_block=DoubleConvDropoutNormNonlin,
        deep_supervision=False,
        use_skip_connections=True,
    ) -> None:
        super().__init__()

        # Task specific
        self.num_classes = output_channels
        self.filters = starting_filters

        # Model parameters
        self.conv_op = conv_op
        self.conv_kwargs = conv_kwargs
        self.norm_op_kwargs = norm_op_kwargs
        self.norm_op = norm_op
        self.dropout_op = dropout_op
        self.dropout_op_kwargs = dropout_op_kwargs
        self.nonlin_kwargs = nonlin_kwargs
        self.nonlin = nonlin
        self.weightInitializer = weightInitializer
        self.basic_block = basic_block
        self.deep_supervision = deep_supervision
        self.use_skip_connections = use_skip_connections

        self.upsample = torch.nn.ConvTranspose3d

        # Decoder
        if not dropout_in_decoder:
            old_dropout_p = self.dropout_op_kwargs["p"]
            self.dropout_op_kwargs["p"] = 0.0

        self.upsample1 = self.upsample(
            self.filters * 16, self.filters * 8, kernel_size=2, stride=2
        )
        self.decoder_conv1 = self.basic_block(
            input_channels=self.filters * (16 if self.use_skip_connections else 8),
            output_channels=self.filters * 8,
            conv_op=self.conv_op,
            conv_kwargs=self.conv_kwargs,
            norm_op=self.norm_op,
            norm_op_kwargs=self.norm_op_kwargs,
            dropout_op=self.dropout_op,
            dropout_op_kwargs=self.dropout_op_kwargs,
            nonlin=self.nonlin,
            nonlin_kwargs=self.nonlin_kwargs,
        )

        self.upsample2 = self.upsample(
            self.filters * 8, self.filters * 4, kernel_size=2, stride=2
        )
        self.decoder_conv2 = self.basic_block(
            input_channels=self.filters * (8 if self.use_skip_connections else 4),
            output_channels=self.filters * 4,
            conv_op=self.conv_op,
            conv_kwargs=self.conv_kwargs,
            norm_op=self.norm_op,
            norm_op_kwargs=self.norm_op_kwargs,
            dropout_op=self.dropout_op,
            dropout_op_kwargs=self.dropout_op_kwargs,
            nonlin=self.nonlin,
            nonlin_kwargs=self.nonlin_kwargs,
        )

        self.upsample3 = self.upsample(
            self.filters * 4, self.filters * 2, kernel_size=2, stride=2
        )
        self.decoder_conv3 = self.basic_block(
            input_channels=self.filters * (4 if self.use_skip_connections else 2),
            output_channels=self.filters * 2,
            conv_op=self.conv_op,
            conv_kwargs=self.conv_kwargs,
            norm_op=self.norm_op,
            norm_op_kwargs=self.norm_op_kwargs,
            dropout_op=self.dropout_op,
            dropout_op_kwargs=self.dropout_op_kwargs,
            nonlin=self.nonlin,
            nonlin_kwargs=self.nonlin_kwargs,
        )

        self.upsample4 = self.upsample(
            self.filters * 2, self.filters, kernel_size=2, stride=2
        )
        self.decoder_conv4 = self.basic_block(
            input_channels=self.filters * (2 if self.use_skip_connections else 1),
            output_channels=self.filters,
            conv_op=self.conv_op,
            conv_kwargs=self.conv_kwargs,
            norm_op=self.norm_op,
            norm_op_kwargs=self.norm_op_kwargs,
            dropout_op=self.dropout_op,
            dropout_op_kwargs=self.dropout_op_kwargs,
            nonlin=self.nonlin,
            nonlin_kwargs=self.nonlin_kwargs,
        )

        self.out_conv = self.conv_op(self.filters, self.num_classes, kernel_size=1)

        if self.deep_supervision:
            self.ds_out_conv0 = self.conv_op(
                self.filters * 16, self.num_classes, kernel_size=1
            )
            self.ds_out_conv1 = self.conv_op(
                self.filters * 8, self.num_classes, kernel_size=1
            )
            self.ds_out_conv2 = self.conv_op(
                self.filters * 4, self.num_classes, kernel_size=1
            )
            self.ds_out_conv3 = self.conv_op(
                self.filters * 2, self.num_classes, kernel_size=1
            )

        if not dropout_in_decoder:
            self.dropout_op_kwargs["p"] = old_dropout_p

        if self.weightInitializer is not None:
            print("initializing weights")
            self.apply(self.weightInitializer)

    def forward(self, xs):
        # We assume xs contains 5 elements. One for each of the skip connections and the bottleneck representation
        # The contents of xs is: [first skip connection, ..., last skip connection, bottleneck]
        assert isinstance(xs, list), type(xs)
        assert len(xs) == 5

        x_enc = xs[4]

        if self.use_skip_connections:
            x5 = torch.cat([self.upsample1(x_enc), xs[3]], dim=1)
            x5 = self.decoder_conv1(x5)

            x6 = torch.cat([self.upsample2(x5), xs[2]], dim=1)
            x6 = self.decoder_conv2(x6)

            x7 = torch.cat([self.upsample3(x6), xs[1]], dim=1)
            x7 = self.decoder_conv3(x7)

            x8 = torch.cat([self.upsample4(x7), xs[0]], dim=1)
            x8 = self.decoder_conv4(x8)
        else:
            x5 = self.decoder_conv1(self.upsample1(x_enc))
            x6 = self.decoder_conv2(self.upsample2(x5))
            x7 = self.decoder_conv3(self.upsample3(x6))
            x8 = self.decoder_conv4(self.upsample4(x7))

        # We only want to do multiple outputs during training, therefore it is only enabled
        # when grad is also enabled because that means we're training. And if for some reason
        # grad is enabled and you're not training, then there's other, bigger problems.
        if self.deep_supervision and torch.is_grad_enabled():
            ds0 = self.ds_out_conv0(xs[4])
            ds1 = self.ds_out_conv1(x5)
            ds2 = self.ds_out_conv2(x6)
            ds3 = self.ds_out_conv3(x7)
            ds4 = self.out_conv(x8)
            return [ds4, ds3, ds2, ds1, ds0]

        logits = self.out_conv(x8)

        return logits


def unet_b(
    mode: str = "segmentation",
    input_channels: int = 1,
    output_channels: int = 1,
):
    return UNet(
        mode=mode,
        input_channels=input_channels,
        output_channels=output_channels,
        use_skip_connections=True,
        starting_filters=32,
    )


def unet_b_lw_dec(
    input_channels: int = 1,
    output_channels: int = 1,
):
    unet_model = UNet(
        input_channels=input_channels,
        output_channels=output_channels,
        decoder_block=MultiLayerConvDropoutNormNonlin.get_block_constructor(1),
        use_skip_connections=False,
        starting_filters=32,
    )

    return unet_model


def unet_xl(
    mode: str = "segmentation",
    input_channels: int = 1,
    output_channels: int = 1,
):
    return UNet(
        input_channels=input_channels,
        output_channels=output_channels,
        mode=mode,
        use_skip_connections=True,
    )


def light_weight_decoder(
    output_channels: int = 1,
    use_skip_connections: bool = False,
    starting_filters: int = 64,
):
    decoder_block = MultiLayerConvDropoutNormNonlin.get_block_constructor(1)
    return UNetDecoder(
        output_channels=output_channels,
        starting_filters=starting_filters,
        use_skip_connections=use_skip_connections,
        basic_block=decoder_block,
    )


def standard_decoder(
    output_channels: int = 1,
    use_skip_connections: bool = False,
    starting_filters: int = 64,
    deep_supervision: bool = False,
):
    decoder_block = MultiLayerConvDropoutNormNonlin.get_block_constructor(2)
    return UNetDecoder(
        output_channels=output_channels,
        starting_filters=starting_filters,
        use_skip_connections=use_skip_connections,
        basic_block=decoder_block,
        deep_supervision=deep_supervision,
    )


def unet_xl_lw_dec(
    input_channels: int = 1,
    output_channels: int = 1,
):
    unet_model = UNet(
        input_channels=input_channels,
        output_channels=output_channels,
        decoder_block=MultiLayerConvDropoutNormNonlin.get_block_constructor(1),
        use_skip_connections=False,
    )

    return unet_model


===============================================
File: utils/__init__.py
===============================================



===============================================
File: utils/data_split.py
===============================================

from typing import Callable, Dict, List
from batchgenerators.utilities.file_and_folder_operations import subfiles, join, save_pickle, isfile, load_pickle


def ensure_splits_contains_split(task_dir: str, train_data_dir: str, split_creator: Callable[[List[str], Dict], Dict]):
    splits_path = join(task_dir, "splits.pkl")
    files = subfiles(train_data_dir, join=False, suffix=".npy")
    assert len(files) > 0

    if isfile(splits_path):
        splits = load_pickle(splits_path)
        splits = split_creator(files, splits)
    else:
        splits = split_creator(files, {})

    save_pickle(splits, splits_path)


def create_combination_split_file(files, splits, prefix1, prefix2):
    key = f"combined_{prefix1}_{prefix2}"
    subkey1 = f"train_{prefix1}_val_{prefix2}"
    subkey2 = f"train_{prefix2}_val_{prefix1}"

    if key in splits.keys() and subkey1 in splits[key].keys() and subkey2 in splits[key].keys():
        return splits
    else:
        if key not in splits.keys():
            splits[key] = {}

        split1 = [file for file in files if file.startswith(prefix1)]
        split2 = [file for file in files if file.startswith(prefix2)]
        assert len(files) == len(split1) + len(split2)

        splits[key][subkey1] = [{"train": split1, "val": split2}]
        splits[key][subkey2] = [{"train": split2, "val": split1}]

        return splits


===============================================
File: utils/masking.py
===============================================

import torch


def generate_random_mask(
    x: torch.Tensor,
    mask_ratio: float,
    patch_size: int,
    out_type: type = int,
):
    # assumes x is (B, C, H, W) or (B, C, H, W, Z)

    dim = len(x.shape) - 2
    assert dim in [2, 3]
    assert x.shape[2] == x.shape[3] if dim == 2 else True
    assert x.shape[2] == x.shape[3] == x.shape[4] if dim == 3 else True

    # check if x.shape is divisible by patch_size
    assert x.shape[2] % patch_size == 0, f"Shape: {x.shape}, Patch size: {patch_size}"

    mask = generate_1d_mask(x, mask_ratio, patch_size, out_type)
    mask = reshape_to_dim(mask, dim)

    up_mask = upsample_mask(mask, patch_size)

    return up_mask


def generate_1d_mask(x: torch.Tensor, mask_ratio: float, patch_size: int, out_type: type):
    assert x.shape[1] in [1, 3], "Channel dim is not 1 or 3. Are you sure?"
    assert out_type in [int, bool]

    N = x.shape[0]
    L = (x.shape[2] // patch_size) ** (len(x.shape) - 2)

    len_keep = int(L * (1 - mask_ratio))

    noise = torch.randn(N, L, device=x.device)

    # sort noise for each sample
    ids_shuffle = torch.argsort(noise, dim=1)
    ids_restore = torch.argsort(ids_shuffle, dim=1)

    # generate the binary mask: 0 is keep 1 is remove
    mask = torch.ones([N, L], device=x.device)
    mask[:, :len_keep] = 0
    # unshuffle to get the binary mask
    mask = torch.gather(mask, dim=1, index=ids_restore)

    if out_type == bool:
        mask = mask.bool()  # (B, H * W)
    elif out_type == int:
        mask = mask.int()

    return mask  # (B, H * W) 0 or False is keep, 1 or True is remove


def reshape_to_dim(mask: torch.Tensor, dim: int):
    assert dim in [2, 3]
    assert len(mask.shape) == 2

    p = round(mask.shape[1] ** (1 / dim))

    if dim == 2:
        return mask.reshape(-1, p, p)
    else:
        return mask.reshape(-1, p, p, p)


def upsample_mask(mask: torch.Tensor, scale: int):
    assert scale > 0
    assert len(mask.shape) in [3, 4]  # (B, H, W) or (B, H, W, Z)

    if len(mask.shape) == 3:
        mask = mask.repeat_interleave(scale, dim=1).repeat_interleave(scale, dim=2)  # (B, H * scale, W * scale)
    else:
        # (B, H * scale, W * scale, Z * scale)
        mask = mask.repeat_interleave(scale, dim=1).repeat_interleave(scale, dim=2).repeat_interleave(scale, dim=3)

    return mask.unsqueeze(1)  # (B, C, H * scale, W * scale) or (B, C, H * scale, W * scale, Z * scale)


===============================================
File: utils/utils.py
===============================================

import os
import logging
import datetime
import torch
import lightning as L
import multiprocessing
from functools import partial
from tqdm import tqdm
from dataclasses import dataclass


@dataclass
class SimplePathConfig:
    """A simplified path configuration for use with Yucca split configuration."""

    train_data_dir: str

    @property
    def task_dir(self) -> str:
        """For compatibility"""
        return self.train_data_dir

    def __init__(self, train_data_dir=None):
        """Initialize with either train_data_dir or task_dir (train_data_dir has priority)."""
        self.train_data_dir = train_data_dir


def setup_seed(continue_from_most_recent=False):
    """Set up a random seed for reproducibility."""
    if not continue_from_most_recent:
        dt = datetime.datetime.now()
        seed = int(dt.strftime("%m%d%H%M%S"))
    else:
        seed = None  # Will be loaded from checkpoint if available

    L.seed_everything(seed=seed, workers=True)
    return torch.initial_seed()


def find_checkpoint(version_dir, continue_from_most_recent):
    """Find the latest checkpoint if continuing training."""
    checkpoint_path = None
    if continue_from_most_recent:
        potential_checkpoint = os.path.join(version_dir, "checkpoints", "last.ckpt")
        if os.path.isfile(potential_checkpoint):
            checkpoint_path = potential_checkpoint
            logging.info(
                "Using last checkpoint and continuing training: %s", checkpoint_path
            )
    return checkpoint_path


def load_pretrained_weights(weights_path, compile_flag):
    """Load pretrained weights with handling for compiled models and PyTorch Lightning checkpoints."""
    checkpoint = torch.load(weights_path, map_location=torch.device("cpu"))

    # Extract the state_dict from PyTorch Lightning checkpoint if needed
    if isinstance(checkpoint, dict) and "state_dict" in checkpoint:
        print("Loading from PyTorch Lightning checkpoint")
        state_dict = checkpoint["state_dict"]
    else:
        print("Loading from standard model checkpoint")
        state_dict = checkpoint

    # Handle compiled checkpoints when loading to uncompiled model
    if isinstance(state_dict, dict) and len(state_dict) > 0:
        first_key = next(iter(state_dict))
        if "_orig_mod" in first_key and not compile_flag:
            print("Converting compiled model weights to uncompiled format")
            uncompiled_state_dict = {}
            for key in state_dict.keys():
                new_key = key.replace("_orig_mod.", "")
                uncompiled_state_dict[new_key] = state_dict[key]
            state_dict = uncompiled_state_dict

    return state_dict


def parallel_process(process_func, tasks, num_workers=None, desc="Processing"):
    """
    Process tasks in parallel using multiprocessing.

    Args:
        process_func: Function that processes a single task
        tasks: List of tasks to process
        num_workers: Number of parallel workers (default: CPU count - 1)
        desc: Description for the progress bar

    Returns:
        List of results from processing each task
    """
    if num_workers is None:
        num_workers = max(1, multiprocessing.cpu_count() - 1)

    print(f"Processing {len(tasks)} items using {num_workers} workers")

    with multiprocessing.Pool(processes=num_workers) as pool:
        results = list(
            tqdm(pool.imap(process_func, tasks), total=len(tasks), desc=desc)
        )

    # Print results summary
    successful = sum(
        1
        for result in results
        if isinstance(result, str) and not result.startswith("Error")
    )
    print(
        f"Processing complete: {successful}/{len(tasks)} items processed successfully"
    )

    # Print any errors
    errors = [
        result
        for result in results
        if isinstance(result, str) and result.startswith("Error")
    ]
    if errors:
        print(f"Encountered {len(errors)} errors:")
        for error in errors[
            :10
        ]:  # Show only first 10 errors to avoid cluttering output
            print(f"  {error}")
        if len(errors) > 10:
            print(f"  ... and {len(errors) - 10} more")

    return results


===============================================
File: utils/visualisation.py
===============================================

from io import BytesIO
from math import ceil, floor
from pathlib import Path
from typing import Optional
import wandb
import torch
import numpy as np
import matplotlib.pyplot as plt
from PIL import Image
import cv2


FPS = 4
DPI = 100


def finish():
    wandb.finish()


def update_config(dct):
    wandb.config.update(dct)


def save_tensor(x, name):
    torch.save(x, name)
    wandb.save(name)
    print(f"Saved {name} to wandb")


def get_gif(x, y, y_hat, slice_dim, version_dir, epoch, desc=""):
    assert version_dir is not None
    assert epoch is not None
    path = f"{version_dir}/gifs"
    Path(path).mkdir(parents=False, exist_ok=True)

    figs = get_figs(x, y, y_hat, slice_dim=slice_dim, n=None, desc=desc)
    frames = []

    for fig in figs:
        buf = BytesIO()
        fig.savefig(buf, format="png", dpi=DPI)
        buf.seek(0)
        frame_arr = np.frombuffer(buf.getvalue(), dtype=np.uint8)
        buf.close()

        frame = cv2.imdecode(frame_arr, 1)
        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)

        plt.close(fig)
        frames.append(Image.fromarray(frame))

    file_name = f"{path}/epoch_{epoch}.gif"
    # Create gif using some PIL magic
    frames[0].save(file_name, save_all=True, append_images=frames[1:], duration=1000 // FPS, loop=0)
    gif = wandb.Video(file_name, format="gif", fps=FPS)

    plt.close("all")

    return [gif]


def get_imgs(x, y, y_hat, slice_dim, n, desc="", idx=0):
    figs = get_figs(x, y, y_hat, slice_dim=slice_dim, n=n, desc=desc, batch_idx=idx)

    def wandb_img(fig):
        img = wandb.Image(fig)
        plt.close(fig)
        return img

    plt.close("all")

    return [wandb_img(fig) for fig in figs]


def get_figs(x: torch.Tensor, y: torch.Tensor, y_hat: torch.Tensor, slice_dim: int, n: Optional[int] = None, desc="", batch_idx: int = 0):
    assert len(x.shape) == 5, x.shape  # (B, 1, X, Y, Z)
    assert len(y.shape) == 5, y.shape  # (B, 1, X, Y, Z)
    assert len(y_hat.shape) == 5, y_hat.shape  # (B, 1, X, Y, Z)

    x = x[batch_idx].squeeze().detach().cpu()
    y = y[batch_idx].squeeze().detach().cpu()
    y_hat = y_hat[batch_idx].squeeze().detach().cpu()

    # we might use mixed precision, so we cast to ensure tensor is compatiable with numpy
    x = x.to(torch.float32)
    y = y.to(torch.float32)
    y_hat = y_hat.to(torch.float32)

    figs = []

    if n is None:
        index_range = torch.arange(x.shape[slice_dim])
    else:
        # we take the middle num_png_images images
        middle = x.shape[slice_dim] // 2
        index_range = torch.arange(middle - floor(n / 2), middle + ceil(n / 2))
        assert len(index_range) == n

    for i in index_range:
        xi = torch.index_select(x, slice_dim, i).squeeze()
        yi = torch.index_select(y, slice_dim, i).squeeze()
        yi_hat = torch.index_select(y_hat, slice_dim, i).squeeze()

        if xi.min() == xi.max() and yi.min() == yi.max():
            continue  # slice is empty

        fig = create_image(xi, yi, yi_hat, rec_text=f"i: {i}", desc=desc)
        figs.append(fig)

    return figs


def create_image(xi, yi, yi_hat, rec_text="", desc=""):
    # x: (X, Y)
    # y: (X, Y)
    # y_hat: (X, Y)

    assert len(xi.shape) == 2, xi.shape
    assert len(yi.shape) == 2, yi.shape
    assert len(yi_hat.shape) == 2, yi_hat.shape

    fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(15, 5), dpi=DPI)

    im = axes[0].imshow(yi, cmap="gray", vmin=0, vmax=1)
    axes[0].set_title("Original")
    fig.colorbar(im, ax=axes[0])

    im = axes[1].imshow(xi, cmap="gray", vmin=0, vmax=1)
    axes[1].set_title("Masked")
    fig.colorbar(im, ax=axes[1])

    im = axes[2].imshow(yi_hat, cmap="gray", vmin=0, vmax=1)
    axes[2].set_title(f"Reconstructed {rec_text}")
    fig.colorbar(im, ax=axes[2])

    fig.text(0.5, 0.05, desc, ha="center")

    return fig

